"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[3853],{3905:(e,a,t)=>{t.d(a,{Zo:()=>u,kt:()=>m});var r=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=r.createContext({}),p=function(e){var a=r.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},u=function(e){var a=p(e.components);return r.createElement(l.Provider,{value:a},e.children)},c="mdxType",k={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},d=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=p(t),d=n,m=c["".concat(l,".").concat(d)]||c[d]||k[d]||o;return t?r.createElement(m,s(s({ref:a},u),{},{components:t})):r.createElement(m,s({ref:a},u))}));function m(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var o=t.length,s=new Array(o);s[0]=d;var i={};for(var l in a)hasOwnProperty.call(a,l)&&(i[l]=a[l]);i.originalType=e,i[c]="string"==typeof e?e:n,s[1]=i;for(var p=2;p<o;p++)s[p]=t[p];return r.createElement.apply(null,s)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},5935:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>k,frontMatter:()=>o,metadata:()=>i,toc:()=>p});var r=t(7462),n=(t(7294),t(3905));const o={title:"Kafka on EKS",sidebar_position:4},s="Apache Kafka",i={unversionedId:"blueprints/streaming-platforms/kafka",id:"blueprints/streaming-platforms/kafka",title:"Kafka on EKS",description:"Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.",source:"@site/docs/blueprints/streaming-platforms/kafka.md",sourceDirName:"blueprints/streaming-platforms",slug:"/blueprints/streaming-platforms/kafka",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/kafka",draft:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/streaming-platforms/kafka.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{title:"Kafka on EKS",sidebar_position:4},sidebar:"blueprints",previous:{title:"Flink Operator on EKS",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/flink"},next:{title:"Apache NiFi on EKS",permalink:"/data-on-eks/docs/blueprints/streaming-platforms/nifi"}},l={},p=[{value:"Strimzi for Apache Kafka",id:"strimzi-for-apache-kafka",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Managed Alternatives",id:"managed-alternatives",level:2},{value:"Amazon Managed Streaming for Apache Kafka (MSK)",id:"amazon-managed-streaming-for-apache-kafka-msk",level:3},{value:"Amazon Kinesis Data Streams (KDS)",id:"amazon-kinesis-data-streams-kds",level:3},{value:"Storage considerations when self-managing Kafka",id:"storage-considerations-when-self-managing-kafka",level:2},{value:"Advantages to using EBS as persistent storage backend",id:"advantages-to-using-ebs-as-persistent-storage-backend",level:3},{value:"What EBS volumes should I use when self-managing Kafka on AWS?",id:"what-ebs-volumes-should-i-use-when-self-managing-kafka-on-aws",level:3},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Deploy",id:"deploy",level:3},{value:"Verify the deployment",id:"verify-the-deployment",level:2},{value:"Create kube config",id:"create-kube-config",level:3},{value:"Get nodes",id:"get-nodes",level:3},{value:"Verify Kafka Brokers and Zookeeper",id:"verify-kafka-brokers-and-zookeeper",level:3},{value:"Verify the running Kafka pods",id:"verify-the-running-kafka-pods",level:3},{value:"Create Kafka Topic and run Sample test",id:"create-kafka-topic-and-run-sample-test",level:2},{value:"Create a kafka Topic",id:"create-a-kafka-topic",level:3},{value:"Execute sample Kafka Producer",id:"execute-sample-kafka-producer",level:3},{value:"Execute sample Kafka Consumer",id:"execute-sample-kafka-consumer",level:3},{value:"Kafka Producer and Consumer output",id:"kafka-producer-and-consumer-output",level:3},{value:"Grafana Dashboard for Kafka",id:"grafana-dashboard-for-kafka",level:2},{value:"Login to Grafana",id:"login-to-grafana",level:3},{value:"Open Strimzi Kafka Dashboard",id:"open-strimzi-kafka-dashboard",level:3},{value:"Open Strimzi Zookeeper Dashboard",id:"open-strimzi-zookeeper-dashboard",level:3},{value:"Open Strimzi Kafka Exporter",id:"open-strimzi-kafka-exporter",level:3},{value:"Cleanup",id:"cleanup",level:2}],u={toc:p},c="wrapper";function k(e){let{components:a,...o}=e;return(0,n.kt)(c,(0,r.Z)({},u,o,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"apache-kafka"},"Apache Kafka"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://kafka.apache.org/"},"Apache Kafka")," is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications."),(0,n.kt)("h2",{id:"strimzi-for-apache-kafka"},"Strimzi for Apache Kafka"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://strimzi.io/"},"Strimzi")," provides a way to run an Apache Kafka cluster on Kubernetes in various deployment configurations.\nStrimzi combines security and simple configuration to deploy and manage Kafka on Kubernetes using kubectl and/or GitOps based on the Operator Pattern."),(0,n.kt)("h2",{id:"architecture"},"Architecture"),(0,n.kt)("admonition",{type:"info"},(0,n.kt)("p",{parentName:"admonition"},"Architecture diagram work in progress")),(0,n.kt)("h2",{id:"managed-alternatives"},"Managed Alternatives"),(0,n.kt)("h3",{id:"amazon-managed-streaming-for-apache-kafka-msk"},"Amazon Managed Streaming for Apache Kafka (MSK)"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://aws.amazon.com/msk/"},"Amazon Managed Streaming for Apache Kafka (Amazon MSK)")," is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data. Amazon MSK provides the control-plane operations, such as those for creating, updating, and deleting clusters. It lets you use Apache Kafka data-plane operations, such as those for producing and consuming data. It runs open-source versions of Apache Kafka. This means existing applications, tooling, and plugins from partners and the Apache Kafka community are supported. You can use Amazon MSK to create clusters that use any of the Apache Kafka versions listed under ",(0,n.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/msk/latest/developerguide/supported-kafka-versions.html"},"Supported Apache Kafka versions"),". Amazon MSK offers cluster-based or serverless deployment types."),(0,n.kt)("h3",{id:"amazon-kinesis-data-streams-kds"},"Amazon Kinesis Data Streams (KDS)"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://aws.amazon.com/kinesis/data-streams/"},"Amazon Kinesis Data Streams (KDS)")," allows users to collect and process large streams of data records in real time. You can create data-processing applications, known as Kinesis Data Streams applications. A typical Kinesis Data Streams application reads data from a data stream as data records. You can send the processed records to dashboards, use them to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services. Kinesis Data Streams support your choice of stream processing framework including Kinesis Client Library (KCL), Apache Flink, and Apache Spark Streaming. It is serverless, and scales automatically."),(0,n.kt)("h2",{id:"storage-considerations-when-self-managing-kafka"},"Storage considerations when self-managing Kafka"),(0,n.kt)("p",null,"The most common resource bottlenecks for Kafka clusters are network throughput, storage throughput, and network throughput between brokers and the storage backend for brokers using network attached storage such as ",(0,n.kt)("a",{parentName:"p",href:"https://aws.amazon.com/ebs/"},"Amazon Elastic Block Store (EBS)"),"."),(0,n.kt)("h3",{id:"advantages-to-using-ebs-as-persistent-storage-backend"},"Advantages to using EBS as persistent storage backend"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Improved flexibility and faster recovery:")," Fault tolerance is commonly achieved via broker (server) replication within\nthe cluster and/or maintaining cross-AZ or region replicas. Since the lifecycle of EBS volumes is independent of\nKafka brokers, if a broker fails and needs to be replaced, the EBS volume attached to the failed broker can be reattached to a replacement broker. Most of the replicated data for the replacement broker is already available in the\nEBS volume, and does not need to be copied over the network from another broker. This avoids most of the\nreplication traffic required to bring the replacement broker up to speed with current operations."),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Just in time scale up:")," The characteristics of EBS volumes can be modified while they\u2019re in use. Broker storage can be\nautomatically scaled over time rather than provisioning storage for peak or adding additional brokers."),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("strong",{parentName:"li"},"Optimized for frequently-accessed-throughput-intensive workloads:")," Volume types such as st1 can be a good fit\nsince these volumes are offered at a relatively low cost, support a large 1 MiB I/O block size, max IOPS of\n500/volume, and includes the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB,\nand a maximum throughput of 500 MB/s per volume.")),(0,n.kt)("h3",{id:"what-ebs-volumes-should-i-use-when-self-managing-kafka-on-aws"},"What EBS volumes should I use when self-managing Kafka on AWS?"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"General purpose SSD volume ",(0,n.kt)("strong",{parentName:"li"},"gp3")," with a balanced price and performance are widely used, and you can ",(0,n.kt)("strong",{parentName:"li"},"independently")," provision storage (up to 16TiB), IOPS (up to 16,000) and throughput (up to 1,000MiB/s)"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"st1")," is a low-cost HDD option for frequently accessed and throughput intensive workloads with up to 500 IOPS and 500 MiB/s"),(0,n.kt)("li",{parentName:"ul"},"For critical applications such as Zookeeper, provisioned IOPS volumes (",(0,n.kt)("strong",{parentName:"li"},"io2 Block Express, io2"),") provide higher durability")),(0,n.kt)("h2",{id:"deploying-the-solution"},"Deploying the Solution"),(0,n.kt)("p",null,"In this ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/awslabs/data-on-eks/tree/main/streaming/kafka"},"example"),", you will provision the following resources to run Kafka Cluster on EKS."),(0,n.kt)("p",null,"This example deploys an EKS Cluster with Kafka into a new VPC."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Creates a new sample VPC, 3 Private Subnets and 3 Public Subnets."),(0,n.kt)("li",{parentName:"ul"},"Creates Internet gateway for Public Subnets and NAT Gateway for Private Subnets."),(0,n.kt)("li",{parentName:"ul"},"Creates EKS Cluster Control plane with public endpoint (for demo reasons only) with two managed node groups."),(0,n.kt)("li",{parentName:"ul"},"Deploys Metrics server, Cluster Autoscaler, self-managed ebs-csi-driver, Strimzi Kafka Operator, Grafana Operator."),(0,n.kt)("li",{parentName:"ul"},"Strimzi Kafka Operator is a Kubernetes Operator for Apache Kafka deployed to ",(0,n.kt)("inlineCode",{parentName:"li"},"strimzi-kafka-operator")," namespace. The operator by default watches and handles ",(0,n.kt)("inlineCode",{parentName:"li"},"kafka")," in all namespaces.")),(0,n.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,n.kt)("p",null,"Ensure that you have installed the following tools on your machine."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"},"aws cli")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("a",{parentName:"li",href:"https://Kubernetes.io/docs/tasks/tools/"},"kubectl")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("a",{parentName:"li",href:"https://learn.hashicorp.com/tutorials/terraform/install-cli"},"terraform"))),(0,n.kt)("h3",{id:"deploy"},"Deploy"),(0,n.kt)("p",null,"Clone the repository"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/awslabs/data-on-eks.git\n")),(0,n.kt)("p",null,"Navigate into one of the example directories and run ",(0,n.kt)("inlineCode",{parentName:"p"},"terraform init")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/streaming/kafka\nterraform init\n")),(0,n.kt)("p",null,"Run Terraform plan to verify the resources created by this execution."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'export AWS_REGION="us-west-2"   # Select your own region\nterraform plan -var="region=$AWS_REGION"\n')),(0,n.kt)("p",null,"Deploy the pattern"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'terraform apply -var="region=$AWS_REGION"\n')),(0,n.kt)("p",null,"Enter ",(0,n.kt)("inlineCode",{parentName:"p"},"yes")," to apply."),(0,n.kt)("admonition",{type:"info"},(0,n.kt)("p",{parentName:"admonition"},"This deployment may take between 20 to 30mins.")),(0,n.kt)("h2",{id:"verify-the-deployment"},"Verify the deployment"),(0,n.kt)("h3",{id:"create-kube-config"},"Create kube config"),(0,n.kt)("p",null,"Create kube config file."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks --region $AWS_REGION update-kubeconfig --name kafka-on-eks\n")),(0,n.kt)("h3",{id:"get-nodes"},"Get nodes"),(0,n.kt)("p",null,"Check if the deployment has created 6 nodes. 3 nodes for Core Node group and 3 for Kafka brokers across 3 AZs."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get nodes\n")),(0,n.kt)("p",null,"Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-text"},"NAME                                        STATUS   ROLES    AGE     VERSION\nip-10-0-10-36.us-west-2.compute.internal    Ready    <none>   5h28m   v1.24.7-eks-fb459a0\nip-10-0-10-47.us-west-2.compute.internal    Ready    <none>   5h20m   v1.24.7-eks-fb459a0\nip-10-0-11-218.us-west-2.compute.internal   Ready    <none>   5h20m   v1.24.7-eks-fb459a0\nip-10-0-11-223.us-west-2.compute.internal   Ready    <none>   5h20m   v1.24.7-eks-fb459a0\nip-10-0-12-202.us-west-2.compute.internal   Ready    <none>   5h20m   v1.24.7-eks-fb459a0\nip-10-0-12-50.us-west-2.compute.internal    Ready    <none>   5h20m   v1.24.7-eks-fb459a0\n")),(0,n.kt)("h3",{id:"verify-kafka-brokers-and-zookeeper"},"Verify Kafka Brokers and Zookeeper"),(0,n.kt)("p",null,"Verify the Kafka Broker and Zookeeper pods and the status created by the Strimzi Operator."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get strimzipodsets.core.strimzi.io -n kafka\n")),(0,n.kt)("p",null,"Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"NAME                PODS   READY PODS   CURRENT PODS   AGE\ncluster-kafka       3      3            3              4h35m\ncluster-zookeeper   3      3            3              4h36m\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get kafka.kafka.strimzi.io -n kafka\n")),(0,n.kt)("p",null,"Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"NAME      DESIRED KAFKA REPLICAS   DESIRED ZK REPLICAS   READY   WARNINGS\ncluster   3                        3                     True\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get kafkatopic.kafka.strimzi.io -n kafka\n")),(0,n.kt)("p",null,"Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"NAME                                                                                               CLUSTER   PARTITIONS   REPLICATION FACTOR   READY\nconsumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a                                        cluster   50           3                    True\nstrimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55                                     cluster   1            3                    True\nstrimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b   cluster   1            3                    True\nstrimzi.cruisecontrol.metrics                                                                      cluster   1            3                    True\nstrimzi.cruisecontrol.modeltrainingsamples                                                         cluster   32           2                    True\nstrimzi.cruisecontrol.partitionmetricsamples                                                       cluster   32           2                    True\n")),(0,n.kt)("h3",{id:"verify-the-running-kafka-pods"},"Verify the running Kafka pods"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods -n kafka\n")),(0,n.kt)("p",null,"Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"NAME                                       READY   STATUS    RESTARTS   AGE\ncluster-cruise-control-79f6457f8d-sm8c2    1/1     Running   0          4h40m\ncluster-entity-operator-5594c965ff-t9nl4   3/3     Running   0          4h40m\ncluster-kafka-0                            1/1     Running   0          4h41m\ncluster-kafka-1                            1/1     Running   0          4h41m\ncluster-kafka-2                            1/1     Running   0          4h41m\ncluster-kafka-exporter-9dbfdff54-wx8vq     1/1     Running   0          4h39m\ncluster-zookeeper-0                        1/1     Running   0          4h42m\ncluster-zookeeper-1                        1/1     Running   0          4h42m\ncluster-zookeeper-2                        1/1     Running   0          4h42m\n")),(0,n.kt)("h2",{id:"create-kafka-topic-and-run-sample-test"},"Create Kafka Topic and run Sample test"),(0,n.kt)("p",null,"We will create one kafka topic and run sample producer script to produce new messages to the kafka topic.\nWe can then verify the data in the topic using sample consumer script."),(0,n.kt)("h3",{id:"create-a-kafka-topic"},"Create a kafka Topic"),(0,n.kt)("p",null,"Run this command to create a new topic called ",(0,n.kt)("inlineCode",{parentName:"p"},"test-topic")," under ",(0,n.kt)("inlineCode",{parentName:"p"},"kafka")," namespace"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"cd streaming/kafka/examples/\nkubectl apply -f kafka-topics.yaml\n")),(0,n.kt)("p",null,"Verify the status of the ",(0,n.kt)("inlineCode",{parentName:"p"},"test-topic")," topic."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl exec -it cluster-kafka-0 -c kafka -n kafka -- /bin/bash -c "/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092"\n')),(0,n.kt)("p",null,"Output"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"__consumer_offsets\n__strimzi-topic-operator-kstreams-topic-store-changelog\n__strimzi_store_topic\nstrimzi.cruisecontrol.metrics\nstrimzi.cruisecontrol.modeltrainingsamples\nstrimzi.cruisecontrol.partitionmetricsamples\ntest-topic\n")),(0,n.kt)("h3",{id:"execute-sample-kafka-producer"},"Execute sample Kafka Producer"),(0,n.kt)("p",null,"Open two terminals one for Kafka producer and one for Kafka Consumer."),(0,n.kt)("p",null,"Execute the following command and press enter twice until you see the ",(0,n.kt)("inlineCode",{parentName:"p"},">")," prompt.\nStart typing some random content. This data will be written to the ",(0,n.kt)("inlineCode",{parentName:"p"},"test-topic"),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list cluster-kafka-bootstrap:9092 --topic test-topic\n")),(0,n.kt)("h3",{id:"execute-sample-kafka-consumer"},"Execute sample Kafka Consumer"),(0,n.kt)("p",null,"Now, you can verify the data written to ",(0,n.kt)("inlineCode",{parentName:"p"},"test-topic")," by running Kafka consumer pod in another terminal"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n kafka run kafka-consumer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server cluster-kafka-bootstrap:9092 --topic test-topic\n")),(0,n.kt)("h3",{id:"kafka-producer-and-consumer-output"},"Kafka Producer and Consumer output"),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"img.png",src:t(146).Z,width:"963",height:"402"})),(0,n.kt)("h2",{id:"grafana-dashboard-for-kafka"},"Grafana Dashboard for Kafka"),(0,n.kt)("h3",{id:"login-to-grafana"},"Login to Grafana"),(0,n.kt)("p",null,"Login to Grafana dashboard by running the following command."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward svc/kube-prometheus-stack-grafana 8080:80 -n kube-prometheus-stack\n")),(0,n.kt)("p",null,"Open browser with local ",(0,n.kt)("a",{parentName:"p",href:"http://localhost:8080/"},"Grafana Web UI")),(0,n.kt)("p",null,"Enter username as ",(0,n.kt)("inlineCode",{parentName:"p"},"admin")," and ",(0,n.kt)("strong",{parentName:"p"},"password")," can be extracted from AWS Secrets Manager with the below command."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'aws secretsmanager get-secret-value \\\n    --secret-id kafka-on-eks-grafana --region $AWS_REGION --query "SecretString" --output text\n')),(0,n.kt)("h3",{id:"open-strimzi-kafka-dashboard"},"Open Strimzi Kafka Dashboard"),(0,n.kt)("p",null,"The below are builtin Kafka dashboards which created during the deployment."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Kafka Brokers Dashboard",src:t(6276).Z,width:"3734",height:"2414"})),(0,n.kt)("h3",{id:"open-strimzi-zookeeper-dashboard"},"Open Strimzi Zookeeper Dashboard"),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Kafka Zookeeper",src:t(7330).Z,width:"3726",height:"2412"})),(0,n.kt)("h3",{id:"open-strimzi-kafka-exporter"},"Open Strimzi Kafka Exporter"),(0,n.kt)("p",null,"You can verify the ",(0,n.kt)("inlineCode",{parentName:"p"},"test-topic")," with three partitions below."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"img.png",src:t(9064).Z,width:"3730",height:"2382"})),(0,n.kt)("h2",{id:"cleanup"},"Cleanup"),(0,n.kt)("p",null,"To clean up your environment, destroy the Terraform modules in reverse order with ",(0,n.kt)("inlineCode",{parentName:"p"},"--target")," option to avoid destroy failures."),(0,n.kt)("p",null,"Destroy the Kubernetes Add-ons, EKS cluster with Node groups and VPC"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},'terraform destroy -target="module.eks_blueprints_kubernetes_addons" -auto-approve\nterraform destroy -target="module.eks_blueprints" -auto-approve\nterraform destroy -target="module.vpc" -auto-approve\n')),(0,n.kt)("p",null,"Finally, destroy any additional resources that are not in the above modules"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"terraform destroy -auto-approve\n")),(0,n.kt)("admonition",{type:"caution"},(0,n.kt)("p",{parentName:"admonition"},"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment"),(0,n.kt)("p",{parentName:"admonition"},"ex. Delete kafka-on-eks EBS volumes")))}k.isMDXComponent=!0},6276:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/kafka-brokers-3ed196a07f8868ea98eea0082008e559.png"},146:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/kafka-consumer-01c14ea820310599340014ad2a739658.png"},9064:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/kafka-exporter-aff2e5b0fd63b3ecb0fbb565ded5f024.png"},7330:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/zookeeper-f584217306081c0f547b33dff0976d59.png"}}]);