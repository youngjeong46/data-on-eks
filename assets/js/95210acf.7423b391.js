"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[679],{3434:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>x,contentTitle:()=>p,default:()=>b,frontMatter:()=>u,metadata:()=>m,toc:()=>g});var s=t(5893),r=t(1151),a=t(3913),o=t(2425),i=t(769),l=t(6112);const d='---\napiVersion: karpenter.sh/v1beta1\nkind: NodePool # Previously kind: Provisioner\nmetadata:\n  name: spark-compute-optimized\n  namespace: karpenter # Same namespace as Karpenter add-on installed\nspec:\n  template:\n    metadata:\n      labels:\n        type: karpenter\n        provisioner: spark-compute-optimized\n        NodeGroupType: SparkComputeOptimized\n    spec:\n      nodeClassRef:\n        name: spark-compute-optimized\n      requirements:\n        - key: "topology.kubernetes.io/zone"\n          operator: In\n          values: [${azs}a] #Update the correct region and zones\n        - key: "karpenter.sh/capacity-type"\n          operator: In\n          values: ["spot", "on-demand"]\n        - key: "kubernetes.io/arch"\n          operator: In\n          values: ["amd64"]\n        - key: "karpenter.k8s.aws/instance-category"\n          operator: In\n          values: ["c"]\n        - key: "karpenter.k8s.aws/instance-family"\n          operator: In\n          values: ["c5d"]\n        - key: "karpenter.k8s.aws/instance-cpu"\n          operator: In\n          values: ["4", "8", "16", "36"]\n        - key: "karpenter.k8s.aws/instance-hypervisor"\n          operator: In\n          values: ["nitro"]\n        - key: "karpenter.k8s.aws/instance-generation"\n          operator: Gt\n          values: ["2"]\n  limits:\n    cpu: 1000\n  disruption:\n    # Describes which types of Nodes Karpenter should consider for consolidation\n    # If using \'WhenUnderutilized\', Karpenter will consider all nodes for consolidation and attempt to remove or replace Nodes when it discovers that the Node is underutilized and could be changed to reduce cost\n    # If using `WhenEmpty`, Karpenter will only consider nodes for consolidation that contain no workload pods\n    consolidationPolicy: WhenEmpty\n    # The amount of time Karpenter should wait after discovering a consolidation decision\n    # This value can currently only be set when the consolidationPolicy is \'WhenEmpty\'\n    # You can choose to disable consolidation entirely by setting the string value \'Never\' here\n    consolidateAfter: 30s\n    # The amount of time a Node can live on the cluster before being removed\n    # Avoiding long-running Nodes helps to reduce security vulnerabilities as well as to reduce the chance of issues that can plague Nodes with long uptimes such as file fragmentation or memory leaks from system processes\n    # You can choose to disable expiration entirely by setting the string value \'Never\' here\n    expireAfter: 720h\n\n  # Priority given to the NodePool when the scheduler considers which NodePool\n  # to select. Higher weights indicate higher priority when comparing NodePools.\n  # Specifying no weight is equivalent to specifying a weight of 0.\n  weight: 10\n\n\n\n# NOTE: Multiple NodePools may point to the same EC2NodeClass.\n---\napiVersion: karpenter.k8s.aws/v1beta1\nkind: EC2NodeClass # Previously kind: AWSNodeTemplate\nmetadata:\n  name: spark-compute-optimized\n  namespace: karpenter\nspec:\n  amiFamily: AL2\n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 50Gi\n        volumeType: gp3\n        encrypted: true\n        deleteOnTermination: true\n  role: "${eks_cluster_id}-karpenter-node"\n  subnetSelectorTerms:\n    - tags: # Update the correct region and zones\n        Name: "${eks_cluster_id}-private*"\n  securityGroupSelectorTerms:\n    - name: "${eks_cluster_id}-node*"\n  userData: |\n    MIME-Version: 1.0\n    Content-Type: multipart/mixed; boundary="BOUNDARY"\n\n    --BOUNDARY\n    Content-Type: text/x-shellscript; charset="us-ascii"\n\n    cat <<-EOF > /etc/profile.d/bootstrap.sh\n    #!/bin/sh\n\n\n    # Configure the NVMe volumes in RAID0 configuration in the bootstrap.sh call.\n    # https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh#L35\n    # This will create a RAID volume and mount it at /mnt/k8s-disks/0\n    #   then mount that volume to /var/lib/kubelet, /var/lib/containerd, and /var/log/pods\n    #   this allows the container daemons and pods to write to the RAID0 by default without needing PersistentVolumes\n    export LOCAL_DISKS=\'raid0\'\n    EOF\n\n    # Source extra environment variables in bootstrap script\n    sed -i \'/^set -o errexit/a\\\\nsource /etc/profile.d/bootstrap.sh\' /etc/eks/bootstrap.sh\n\n    --BOUNDARY--\n  tags:\n    InstanceType: "spark-compute-optimized"    # optional, add tags for your own use\n',c='---\napiVersion: karpenter.sh/v1beta1\nkind: NodePool # Previously kind: Provisioner\nmetadata:\n  name: spark-memory-optimized\n  namespace: karpenter # Same namespace as Karpenter add-on installed\nspec:\n  template:\n    metadata:\n      labels:\n        type: karpenter\n        provisioner: spark-memory-optimized\n        NodeGroupType: SparkMemoryOptimized\n    spec:\n      nodeClassRef:\n        name: spark-memory-optimized\n      requirements:\n        - key: "topology.kubernetes.io/zone"\n          operator: In\n          values: [${azs}a] #Update the correct region and zones\n        - key: "karpenter.sh/capacity-type"\n          operator: In\n          values: ["spot", "on-demand"]\n        - key: "kubernetes.io/arch"\n          operator: In\n          values: ["amd64"]\n        - key: "karpenter.k8s.aws/instance-category"\n          operator: In\n          values: ["r"]\n        - key: "karpenter.k8s.aws/instance-family"\n          operator: In\n          values: ["r5d"]\n        - key: "karpenter.k8s.aws/instance-cpu"\n          operator: In\n          values: ["4", "8", "16", "32"]\n        - key: "karpenter.k8s.aws/instance-hypervisor"\n          operator: In\n          values: ["nitro"]\n        - key: "karpenter.k8s.aws/instance-generation"\n          operator: Gt\n          values: ["2"]\n  limits:\n    cpu: 1000\n  disruption:\n    # Describes which types of Nodes Karpenter should consider for consolidation\n    # If using \'WhenUnderutilized\', Karpenter will consider all nodes for consolidation and attempt to remove or replace Nodes when it discovers that the Node is underutilized and could be changed to reduce cost\n    # If using `WhenEmpty`, Karpenter will only consider nodes for consolidation that contain no workload pods\n    consolidationPolicy: WhenEmpty\n    # The amount of time Karpenter should wait after discovering a consolidation decision\n    # This value can currently only be set when the consolidationPolicy is \'WhenEmpty\'\n    # You can choose to disable consolidation entirely by setting the string value \'Never\' here\n    consolidateAfter: 30s\n    # The amount of time a Node can live on the cluster before being removed\n    # Avoiding long-running Nodes helps to reduce security vulnerabilities as well as to reduce the chance of issues that can plague Nodes with long uptimes such as file fragmentation or memory leaks from system processes\n    # You can choose to disable expiration entirely by setting the string value \'Never\' here\n    expireAfter: 720h\n\n  # Priority given to the NodePool when the scheduler considers which NodePool\n  # to select. Higher weights indicate higher priority when comparing NodePools.\n  # Specifying no weight is equivalent to specifying a weight of 0.\n  weight: 10\n\n\n\n# NOTE: Multiple NodePools may point to the same EC2NodeClass.\n---\napiVersion: karpenter.k8s.aws/v1beta1\nkind: EC2NodeClass # Previously kind: AWSNodeTemplate\nmetadata:\n  name: spark-memory-optimized\n  namespace: karpenter\nspec:\n  amiFamily: AL2\n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 100Gi\n        volumeType: gp3\n        encrypted: true\n        deleteOnTermination: true\n  role: "${eks_cluster_id}-karpenter-node"\n  subnetSelectorTerms:\n    - tags: # Update the correct region and zones\n        Name: "${eks_cluster_id}-private*"\n  securityGroupSelectorTerms:\n    - name: "${eks_cluster_id}-node*"\n  userData: |\n    MIME-Version: 1.0\n    Content-Type: multipart/mixed; boundary="BOUNDARY"\n\n    --BOUNDARY\n    Content-Type: text/x-shellscript; charset="us-ascii"\n\n    cat <<-EOF > /etc/profile.d/bootstrap.sh\n    #!/bin/sh\n\n\n    # Configure the NVMe volumes in RAID0 configuration in the bootstrap.sh call.\n    # https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh#L35\n    # This will create a RAID volume and mount it at /mnt/k8s-disks/0\n    #   then mount that volume to /var/lib/kubelet, /var/lib/containerd, and /var/log/pods\n    #   this allows the container daemons and pods to write to the RAID0 by default without needing PersistentVolumes\n    export LOCAL_DISKS=\'raid0\'\n    EOF\n\n    # Source extra environment variables in bootstrap script\n    sed -i \'/^set -o errexit/a\\\\nsource /etc/profile.d/bootstrap.sh\' /etc/eks/bootstrap.sh\n\n    --BOUNDARY--\n  tags:\n    InstanceType: "spark-memory-optimized"    # optional, add tags for your own use\n',h='---\napiVersion: karpenter.sh/v1beta1\nkind: NodePool # Previously kind: Provisioner\nmetadata:\n  name: spark-graviton-memory-optimized\n  namespace: karpenter # Same namespace as Karpenter add-on installed\nspec:\n  template:\n    metadata:\n      labels:\n        type: karpenter\n        provisioner: spark-graviton-memory-optimized\n        NodeGroupType: SparkGravitonMemoryOptimized\n    spec:\n      nodeClassRef:\n        name: spark-graviton-memory-optimized\n      requirements:\n        - key: "topology.kubernetes.io/zone"\n          operator: In\n          values: [${azs}a] #Update the correct region and zones\n        - key: "karpenter.sh/capacity-type"\n          operator: In\n          values: ["spot", "on-demand"]\n        - key: "kubernetes.io/arch"\n          operator: In\n          values: ["arm64"]\n        - key: "karpenter.k8s.aws/instance-category"\n          operator: In\n          values: ["r"]\n        - key: "karpenter.k8s.aws/instance-family"\n          operator: In\n          values: ["r6gd"]\n        - key: "karpenter.k8s.aws/instance-cpu"\n          operator: In\n          values: ["4", "8", "16", "32"]\n        - key: "karpenter.k8s.aws/instance-hypervisor"\n          operator: In\n          values: ["nitro"]\n        - key: "karpenter.k8s.aws/instance-generation"\n          operator: Gt\n          values: ["2"]\n  limits:\n    cpu: 1000\n  disruption:\n    # Describes which types of Nodes Karpenter should consider for consolidation\n    # If using \'WhenUnderutilized\', Karpenter will consider all nodes for consolidation and attempt to remove or replace Nodes when it discovers that the Node is underutilized and could be changed to reduce cost\n    # If using `WhenEmpty`, Karpenter will only consider nodes for consolidation that contain no workload pods\n    consolidationPolicy: WhenEmpty\n    # The amount of time Karpenter should wait after discovering a consolidation decision\n    # This value can currently only be set when the consolidationPolicy is \'WhenEmpty\'\n    # You can choose to disable consolidation entirely by setting the string value \'Never\' here\n    consolidateAfter: 30s\n    # The amount of time a Node can live on the cluster before being removed\n    # Avoiding long-running Nodes helps to reduce security vulnerabilities as well as to reduce the chance of issues that can plague Nodes with long uptimes such as file fragmentation or memory leaks from system processes\n    # You can choose to disable expiration entirely by setting the string value \'Never\' here\n    expireAfter: 720h\n\n  # Priority given to the NodePool when the scheduler considers which NodePool\n  # to select. Higher weights indicate higher priority when comparing NodePools.\n  # Specifying no weight is equivalent to specifying a weight of 0.\n  weight: 10\n\n\n\n# NOTE: Multiple NodePools may point to the same EC2NodeClass.\n---\napiVersion: karpenter.k8s.aws/v1beta1\nkind: EC2NodeClass # Previously kind: AWSNodeTemplate\nmetadata:\n  name: spark-graviton-memory-optimized\n  namespace: karpenter\nspec:\n  amiFamily: AL2\n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 100Gi\n        volumeType: gp3\n        encrypted: true\n        deleteOnTermination: true\n  role: "${eks_cluster_id}-karpenter-node"\n  subnetSelectorTerms:\n    - tags: # Update the correct region and zones\n        Name: "${eks_cluster_id}-private*"\n  securityGroupSelectorTerms:\n    - name: "${eks_cluster_id}-node*"\n  userData: |\n    MIME-Version: 1.0\n    Content-Type: multipart/mixed; boundary="BOUNDARY"\n\n    --BOUNDARY\n    Content-Type: text/x-shellscript; charset="us-ascii"\n\n    cat <<-EOF > /etc/profile.d/bootstrap.sh\n    #!/bin/sh\n\n\n    # Configure the NVMe volumes in RAID0 configuration in the bootstrap.sh call.\n    # https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh#L35\n    # This will create a RAID volume and mount it at /mnt/k8s-disks/0\n    #   then mount that volume to /var/lib/kubelet, /var/lib/containerd, and /var/log/pods\n    #   this allows the container daemons and pods to write to the RAID0 by default without needing PersistentVolumes\n    export LOCAL_DISKS=\'raid0\'\n    EOF\n\n    # Source extra environment variables in bootstrap script\n    sed -i \'/^set -o errexit/a\\\\nsource /etc/profile.d/bootstrap.sh\' /etc/eks/bootstrap.sh\n\n    --BOUNDARY--\n  tags:\n    InstanceType: "spark-graviton-memory-optimized"    # optional, add tags for your own use\n',u={sidebar_position:2,sidebar_label:"EMR on EKS with Karpenter"},p="EMR on EKS with Karpenter",m={id:"blueprints/amazon-emr-on-eks/emr-eks-karpenter",title:"EMR on EKS with [Karpenter](https://karpenter.sh/)",description:"Introduction",source:"@site/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter.md",sourceDirName:"blueprints/amazon-emr-on-eks",slug:"/blueprints/amazon-emr-on-eks/emr-eks-karpenter",permalink:"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter",draft:!1,unlisted:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/amazon-emr-on-eks/emr-eks-karpenter.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,sidebar_label:"EMR on EKS with Karpenter"},sidebar:"blueprints",previous:{title:"Introduction",permalink:"/data-on-eks/docs/blueprints/amazon-emr-on-eks/"},next:{title:"EMR on EKS Observability",permalink:"/data-on-eks/docs/blueprints/amazon-emr-on-eks/emr-eks-observability"}},x={},g=[{value:"Introduction",id:"introduction",level:2},{value:"Customizing Add-ons",id:"customizing-add-ons",level:3},{value:"Prerequisites:",id:"prerequisites",level:3},{value:"Verify the resources",id:"verify-the-resources",level:3},{value:"Run Sample Spark job",id:"run-sample-spark-job",level:2},{value:"Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter Nodepool",id:"execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-nodepool",level:3},{value:"Running Sample Spark job using FSx for Lustre",id:"running-sample-spark-job-using-fsx-for-lustre",level:3},{value:"Running Sample Spark job using Apache YuniKorn Batch Scheduler",id:"running-sample-spark-job-using-apache-yunikorn-batch-scheduler",level:3},{value:"Prerequisites:",id:"prerequisites-1",level:3},{value:"Run Interactive Workload with Managed Endpoint",id:"run-interactive-workload-with-managed-endpoint",level:2},{value:"Creating a managed endpoint",id:"creating-a-managed-endpoint",level:3},{value:"Cleanup of Endpoint resources",id:"cleanup-of-endpoint-resources",level:3},{value:"Cleanup",id:"cleanup",level:2}];function f(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.a)(),...e.components},{Details:u}=n;return u||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.h1,{id:"emr-on-eks-with-karpenter",children:["EMR on EKS with ",(0,s.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})]}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsxs)(n.p,{children:["In this ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/data-on-eks/tree/main/analytics/terraform/emr-eks-karpenter",children:"pattern"}),", you will deploy an EMR on EKS cluster and use ",(0,s.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," Nodepools for scaling Spark jobs."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),"\n",(0,s.jsx)(n.img,{alt:"emr-eks-karpenter",src:t(5039).Z+"",width:"992",height:"566"})]}),"\n",(0,s.jsx)(n.p,{children:"This pattern uses opinionated defaults to keep the deployment experience simple but also keeps it flexible so that you can pick and choose necessary add-ons during deployment. We recommend keeping the defaults if you are new to EMR on EKS and only customize if you have viable alternative option available for replacement."}),"\n",(0,s.jsx)(n.p,{children:"In terms of infrastructure, here are the resources that are created by this pattern"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Creates an EKS Cluster Control plane with public endpoint (recommended for demo/poc environment)"}),"\n",(0,s.jsxs)(n.li,{children:["One managed node group","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Core Node group with 3 instances spanning multi-AZs for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, Observability, Logging etc."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["Enables EMR on EKS","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Creates two namespaces (",(0,s.jsx)(n.code,{children:"emr-data-team-a"}),", ",(0,s.jsx)(n.code,{children:"emr-data-team-b"}),") for data teams"]}),"\n",(0,s.jsxs)(n.li,{children:["Creates Kubernetes role and role binding(",(0,s.jsx)(n.code,{children:"emr-containers"})," user) for both namespaces"]}),"\n",(0,s.jsx)(n.li,{children:"IAM roles for both teams needed for job execution"}),"\n",(0,s.jsxs)(n.li,{children:["Update ",(0,s.jsx)(n.code,{children:"AWS_AUTH"})," config map with ",(0,s.jsx)(n.code,{children:"emr-containers"})," user and ",(0,s.jsx)(n.code,{children:"AWSServiceRoleForAmazonEMRContainers"})," role"]}),"\n",(0,s.jsx)(n.li,{children:"Create a trust relationship between the job execution role and the identity of the EMR managed service account"}),"\n",(0,s.jsxs)(n.li,{children:["Create EMR Virtual Cluster for ",(0,s.jsx)(n.code,{children:"emr-data-team-a"})," & ",(0,s.jsx)(n.code,{children:"emr-data-team-b"})," and IAM policies for both"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"You can see the list of add-ons available below."}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:["We recommend running all the default system add-ons on a dedicated EKS managed nodegroup such as ",(0,s.jsx)(n.code,{children:"core-node-group"})," as provided by this pattern."]})}),"\n",(0,s.jsx)(n.admonition,{type:"danger",children:(0,s.jsxs)(n.p,{children:["We don't recommend removing critical add-ons (",(0,s.jsx)(n.code,{children:"Amazon VPC CNI"}),", ",(0,s.jsx)(n.code,{children:"CoreDNS"}),", ",(0,s.jsx)(n.code,{children:"Kube-proxy"}),")."]})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Add-on"}),(0,s.jsx)(n.th,{style:{textAlign:"center"},children:"Enabled by default?"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Benefits"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Link"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Amazon VPC CNI"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:["VPC CNI is available as an ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html",children:"EKS add-on"})," and is responsible for creating ENI's and IPv4 or IPv6 addresses for your spark application pods"]}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html",children:"VPC CNI Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"CoreDNS"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:["CoreDNS is available as an ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html",children:"EKS add-on"})," and is responsible for resolving DNS queries for spark application and for Kubernetes cluster"]}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html",children:"EKS CoreDNS Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Kube-proxy"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:["Kube-proxy is available as an ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html",children:"EKS add-on"})," and it maintains network rules on your nodes and enables network communication to your spark application pods"]}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html",children:"EKS kube-proxy Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Amazon EBS CSI driver"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:["EBS CSI driver is available as an ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/eks-networking-add-ons.html",children:"EKS add-on"})," and it allows EKS clusters to manage the lifecycle of EBS volumes"]}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html",children:"EBS CSI Driver Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Karpenter"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Karpenter is nodegroup-less autoscaler that provides just-in-time compute capacity for spark applications on Kubernetes clusters"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Cluster Autoscaler"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:["Kubernetes Cluster Autoscaler automatically adjusts the size of Kubernetes cluster and is available for scaling nodegroups (such as ",(0,s.jsx)(n.code,{children:"core-node-group"}),") in the cluster"]}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md",children:"Cluster Autoscaler Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Cluster proportional autoscaler"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"This is responsible for scaling CoreDNS pods in your Kubernetes cluster"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://github.com/kubernetes-sigs/cluster-proportional-autoscaler",children:"Cluster Proportional Autoscaler Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Metrics server"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Kubernetes metrics server is responsible for aggregating cpu, memory and other container resource usage within your cluster"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html",children:"EKS Metrics Server Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Prometheus"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Prometheus is responsible for monitoring EKS cluster including spark applications in your EKS cluster. We use Prometheus deployment for scraping and ingesting metrics into Amazon Managed Prometheus and Kubecost"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://prometheus.io/docs/introduction/overview/",children:"Prometheus Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Amazon Managed Prometheus"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"This is responsible for storing and scaling of EKS cluster and spark application metrics"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/prometheus/latest/userguide/what-is-Amazon-Managed-Service-Prometheus.html",children:"Amazon Managed Prometheus Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Kubecost"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"Yes"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Kubecost is responsible for providing cost break down by Spark application. You can monitor costs based on per job, namespace or labels"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/cost-monitoring.html",children:"EKS Kubecost Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"CloudWatch metrics"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"No"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"CloudWatch container insights metrics shows simple and standardized way to monitor not only AWS resources but also EKS resources on CloudWatch dashboard"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-metrics-EKS.html",children:"CloudWatch Container Insights Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"AWS for Fluent-bit"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"No"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"This can be used to publish EKS cluster and worker node logs to CloudWatch Logs or 3rd party logging system"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://github.com/aws/aws-for-fluent-bit",children:"AWS For Fluent-bit Documentation"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"FSx for Lustre CSI driver"}),(0,s.jsx)(n.td,{style:{textAlign:"center"},children:"No"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"This can be used for running Spark application using FSx for Lustre"}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html",children:"FSx for Lustre CSI Driver Documentation"})})]})]})]}),"\n",(0,s.jsxs)(i.Z,{header:(0,s.jsx)(n.h3,{children:(0,s.jsx)(n.span,{children:"Customizing Add-ons"})}),children:[(0,s.jsx)(n.h3,{id:"customizing-add-ons",children:"Customizing Add-ons"}),(0,s.jsxs)(n.p,{children:["You can customize your deployment anytime either by changing recommended system add-ons in ",(0,s.jsx)(n.code,{children:"addons.tf"})," or by changing optional add-ons in ",(0,s.jsx)(n.code,{children:"variables.tf"}),"."]}),(0,s.jsxs)(n.p,{children:["For example, let's say you want to remove Amazon Managed Prometheus because you have another application that captures Prometheus metrics, you can edit ",(0,s.jsx)(n.code,{children:"addons.tf"})," using your favorite editor, find Amazon Managed Prometheus and change to ",(0,s.jsx)(n.code,{children:"false"})]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'  enable_prometheus = false\n  prometheus_helm_config = {\n    name       = "prometheus"\n    repository = "https://prometheus-community.github.io/helm-charts"\n    chart      = "prometheus"\n    version    = "15.10.1"\n    namespace  = "prometheus"\n    timeout    = "300"\n    values     = [templatefile("${path.module}/helm-values/prometheus-values.yaml", {})]\n  }\n'})}),(0,s.jsxs)(n.p,{children:["If you want to use FSx for Lustre storage while running Spark application for storing shuffle files or accessing data from S3, you can install FSx CSI driver by searching for FSx in ",(0,s.jsx)(n.code,{children:"variables.tf"})," and edit the file"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'variable "enable_fsx_for_lustre" {\n  default     = false\n  description = "Deploys fsx for lustre addon, storage class and static FSx for Lustre filesystem for EMR"\n  type        = bool\n}\n'})}),(0,s.jsxs)(n.p,{children:["Once the changes are saved, follow the ",(0,s.jsx)(n.a,{href:"#deploying-the-solution",children:"deployment guide"})," if this is a new installation or apply these changes using Terraform for existing installation"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"terraform apply\n"})})]}),"\n",(0,s.jsxs)(i.Z,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Deploying the Solution"})}),children:[(0,s.jsx)(n.p,{children:"Let's go through the deployment steps"}),(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites:"}),(0,s.jsx)(n.p,{children:"Ensure that you have installed the following tools on your machine."}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html",children:"aws cli"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://Kubernetes.io/docs/tasks/tools/",children:"kubectl"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"terraform"})}),"\n"]}),(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.em,{children:["Note: Currently Amazon Managed Prometheus supported only in selected regions. Please see this ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/prometheus/latest/userguide/what-is-Amazon-Managed-Service-Prometheus.html",children:"userguide"})," for supported regions."]})}),(0,s.jsx)(n.p,{children:"First, clone the repository"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/data-on-eks.git\n"})}),(0,s.jsxs)(n.p,{children:["Navigate into one of the example directories and run ",(0,s.jsx)(n.code,{children:"install.sh"})," script"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/analytics/terraform/emr-eks-karpenter\nchmod +x install.sh\n./install.sh\n"})}),(0,s.jsx)(n.h3,{id:"verify-the-resources",children:"Verify the resources"}),(0,s.jsx)(n.p,{children:"Verify the Amazon EKS Cluster and Amazon Managed service for Prometheus"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws eks describe-cluster --name emr-eks-karpenter\n\naws amp list-workspaces --alias amp-ws-emr-eks-karpenter\n"})}),(0,s.jsxs)(n.p,{children:["Verify EMR on EKS Namespaces ",(0,s.jsx)(n.code,{children:"emr-data-team-a"})," and ",(0,s.jsx)(n.code,{children:"emr-data-team-b"})," and Pod status for ",(0,s.jsx)(n.code,{children:"Prometheus"}),", ",(0,s.jsx)(n.code,{children:"Vertical Pod Autoscaler"}),", ",(0,s.jsx)(n.code,{children:"Metrics Server"})," and ",(0,s.jsx)(n.code,{children:"Cluster Autoscaler"}),"."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws eks --region us-west-2 update-kubeconfig --name emr-eks-karpenter # Creates k8s config file to authenticate with EKS Cluster\n\nkubectl get nodes # Output shows the EKS Managed Node group nodes\n\nkubectl get ns | grep emr-data-team # Output shows emr-data-team-a and emr-data-team-b namespaces for data teams\n\nkubectl get pods --namespace=prometheus # Output shows Prometheus server and Node exporter pods\n\nkubectl get pods --namespace=vpa  # Output shows Vertical Pod Autoscaler pods\n\nkubectl get pods --namespace=kube-system | grep  metrics-server # Output shows Metric Server pod\n\nkubectl get pods --namespace=kube-system | grep  cluster-autoscaler # Output shows Cluster Autoscaler pod\n"})})]}),"\n",(0,s.jsx)(n.h2,{id:"run-sample-spark-job",children:"Run Sample Spark job"}),"\n",(0,s.jsxs)(n.p,{children:["The pattern shows how to run spark jobs in a multi-tenant EKS cluster. The examples showcases two data teams using namespaces ",(0,s.jsx)(n.code,{children:"emr-data-team-a"})," and ",(0,s.jsx)(n.code,{children:"emr-data-team-b"})," mapped to their EMR virtual clusters. You can use different Karpenter Nodepools for each team so that they can submit jobs that are unique to their workload. Teams can also use different storage requirements to run their Spark jobs. For example, you can use compute optimized Nodepool that has ",(0,s.jsx)(n.code,{children:"taints"})," and specify ",(0,s.jsx)(n.code,{children:"tolerations"})," using pod templates so that you can run spark on compute optimized EC2 instances. In terms of storage, you can decide whether to use ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html",children:"EC2 instance-store"})," or ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html",children:"EBS"})," or ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html",children:"FSx for lustre"})," volumes for data processing. The default storage that is used in these examples is EC2 instance store because of performance benefit"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"spark-compute-optimized"})," Nodepool to run spark jobs on ",(0,s.jsx)(n.code,{children:"c5d"})," instances."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"spark-memory-optimized"})," Nodepool to run spark jobs on ",(0,s.jsx)(n.code,{children:"r5d"})," instances."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"spark-graviton-memory-optimized"})," Nodepool to run spark jobs on ",(0,s.jsx)(n.code,{children:"r6gd"})," Graviton instances(",(0,s.jsx)(n.code,{children:"ARM64"}),")."]}),"\n"]}),"\n",(0,s.jsxs)(a.Z,{children:[(0,s.jsxs)(o.Z,{value:"spark-compute-optimized",lebal:"spark-compute-optimized",default:!0,children:[(0,s.jsx)(n.p,{children:"In this tutorial, you will use Karpenter Nodepool that uses compute optimized instances. This template leverages the Karpenter AWSNodeTemplates."}),(0,s.jsxs)(u,{children:[(0,s.jsx)("summary",{children:" To view Karpenter Nodepool for compute optimized instances, Click to toggle content!"}),(0,s.jsx)(l.Z,{language:"yaml",children:d})]}),(0,s.jsxs)(n.p,{children:["To run Spark Jobs that can use this Nodepool, you need to submit your jobs by adding ",(0,s.jsx)(n.code,{children:"tolerations"})," to your pod templates"]}),(0,s.jsx)(n.p,{children:"For example,"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'spec:\n  tolerations:\n    - key: "spark-compute-optimized"\n      operator: "Exists"\n      effect: "NoSchedule"\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Execute the sample PySpark Job to trigger compute optimized Karpenter Nodepool"})}),(0,s.jsxs)(n.p,{children:["The following script requires four input parameters ",(0,s.jsx)(n.code,{children:"virtual_cluster_id"}),", ",(0,s.jsx)(n.code,{children:"job_execution_role_arn"}),", ",(0,s.jsx)(n.code,{children:"cloudwatch_log_group_name"})," & ",(0,s.jsx)(n.code,{children:"S3_Bucket"})," to store PySpark scripts, Pod templates and Input data. You can get these values ",(0,s.jsx)(n.code,{children:"terraform apply"})," output values or by running ",(0,s.jsx)(n.code,{children:"terraform output"}),". For ",(0,s.jsx)(n.code,{children:"S3_BUCKET"}),", Either create a new S3 bucket or use an existing S3 bucket."]}),(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsx)(n.p,{children:"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job."})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-compute-provisioner/\n./execute_emr_eks_job.sh\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3\nEnter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a\nEnter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a\nEnter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://<bucket-name>: s3://example-bucket\n"})}),(0,s.jsx)(n.p,{children:"Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verify the job execution"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods --namespace=emr-data-team-a -w\n"})})]}),(0,s.jsxs)(o.Z,{value:"spark-memory-optimized",label:"spark-memory-optimized",children:[(0,s.jsx)(n.p,{children:"In this tutorial, you will use Karpenter Nodepool that uses memory optimized instances. This template uses the AWS Node template with Userdata."}),(0,s.jsxs)(u,{children:[(0,s.jsx)("summary",{children:" To view Karpenter Nodepool for memory optimized instances, Click to toggle content!"}),(0,s.jsx)(l.Z,{language:"yaml",children:c})]}),(0,s.jsxs)(n.p,{children:["To run Spark Jobs that can use this Nodepool, you need to submit your jobs by adding ",(0,s.jsx)(n.code,{children:"tolerations"})," to your pod templates"]}),(0,s.jsx)(n.p,{children:"For example,"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'spec:\n  tolerations:\n    - key: "spark-memory-optimized"\n      operator: "Exists"\n      effect: "NoSchedule"\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Execute the sample PySpark Job to trigger memory optimized Karpenter Nodepool"})}),(0,s.jsxs)(n.p,{children:["The following script requires four input parameters ",(0,s.jsx)(n.code,{children:"virtual_cluster_id"}),", ",(0,s.jsx)(n.code,{children:"job_execution_role_arn"}),", ",(0,s.jsx)(n.code,{children:"cloudwatch_log_group_name"})," & ",(0,s.jsx)(n.code,{children:"S3_Bucket"})," to store PySpark scripts, Pod templates and Input data. You can get these values ",(0,s.jsx)(n.code,{children:"terraform apply"})," output values or by running ",(0,s.jsx)(n.code,{children:"terraform output"}),". For ",(0,s.jsx)(n.code,{children:"S3_BUCKET"}),", Either create a new S3 bucket or use an existing S3 bucket."]}),(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsx)(n.p,{children:"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job."})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-memory-provisioner/\n./execute_emr_eks_job.sh\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3\nEnter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a\nEnter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a\nEnter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://<bucket-name>: s3://example-bucket\n"})}),(0,s.jsx)(n.p,{children:"Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verify the job execution"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods --namespace=emr-data-team-a -w\n"})})]}),(0,s.jsxs)(o.Z,{value:"spark-graviton-memory-optimized",label:"spark-graviton-memory-optimized",children:[(0,s.jsx)(n.p,{children:"In this tutorial, you will use Karpenter Nodepool that uses Graviton memory optimized instances. This template uses the AWS Node template with Userdata."}),(0,s.jsxs)(u,{children:[(0,s.jsx)("summary",{children:" To view Karpenter Nodepool for Graviton memory optimized instances, Click to toggle content!"}),(0,s.jsx)(l.Z,{language:"yaml",children:h})]}),(0,s.jsxs)(n.p,{children:["To run Spark Jobs that can use this Nodepool, you need to submit your jobs by adding ",(0,s.jsx)(n.code,{children:"tolerations"})," to your pod templates"]}),(0,s.jsx)(n.p,{children:"For example,"}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'spec:\n  tolerations:\n    - key: "spark-graviton-memory-optimized"\n      operator: "Exists"\n      effect: "NoSchedule"\n'})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Execute the sample PySpark Job to trigger Graviton memory optimized Karpenter Nodepool"})}),(0,s.jsxs)(n.p,{children:["The following script requires four input parameters ",(0,s.jsx)(n.code,{children:"virtual_cluster_id"}),", ",(0,s.jsx)(n.code,{children:"job_execution_role_arn"}),", ",(0,s.jsx)(n.code,{children:"cloudwatch_log_group_name"})," & ",(0,s.jsx)(n.code,{children:"S3_Bucket"})," to store PySpark scripts, Pod templates and Input data. You can get these values ",(0,s.jsx)(n.code,{children:"terraform apply"})," output values or by running ",(0,s.jsx)(n.code,{children:"terraform output"}),". For ",(0,s.jsx)(n.code,{children:"S3_BUCKET"}),", Either create a new S3 bucket or use an existing S3 bucket."]}),(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsx)(n.p,{children:"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job."})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-graviton-memory-provisioner/\n./execute_emr_eks_job.sh\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Enter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3\nEnter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a\nEnter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a\nEnter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://<bucket-name>: s3://example-bucket\n"})}),(0,s.jsx)(n.p,{children:"Karpenter may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verify the job execution"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods --namespace=emr-data-team-a -w\n"})})]})]}),"\n",(0,s.jsx)(n.h3,{id:"execute-the-sample-pyspark-job-that-uses-ebs-volumes-and-compute-optimized-karpenter-nodepool",children:"Execute the sample PySpark job that uses EBS volumes and compute optimized Karpenter Nodepool"}),"\n",(0,s.jsx)(n.p,{children:"This pattern uses EBS volumes for data processing and compute optimized Nodepool. You can modify the Nodepool by changing nodeselector in driver and executor pod templates. In order to change Nodepools, simply update your pod templates to desired Nodepool"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'  nodeSelector:\n    NodeGroupType: "SparkComputeOptimized"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["You can also update ",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/ec2/instance-types/#Compute_Optimized",children:"EC2 instances"})," that doesn't include instance store volumes (for example c5.xlarge) and remove c5d's if needed for this exercise"]}),"\n",(0,s.jsx)(n.p,{children:"We will create Storageclass that will be used by drivers and executors. We'll create static Persistent Volume Claim (PVC) for the driver pod but we'll use dynamically created ebs volumes for executors."}),"\n",(0,s.jsx)(n.p,{children:"Create StorageClass and PVC using example provided"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/\nkubectl apply -f ebs-storageclass-pvc.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"Let's run the job"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd data-on-eks/analytics/terraform/emr-eks-karpenter/examples/ebs-pvc/karpenter-compute-provisioner-ebs/\n./execute_emr_eks_job.sh\nEnter the EMR Virtual Cluster ID: 4ucrncg6z4nd19vh1lidna2b3\nEnter the EMR Execution Role ARN: arn:aws:iam::123456789102:role/emr-eks-karpenter-emr-eks-data-team-a\nEnter the CloudWatch Log Group name: /emr-on-eks-logs/emr-eks-karpenter/emr-data-team-a\nEnter the S3 Bucket for storing PySpark Scripts, Pod Templates and Input data. For e.g., s3://<bucket-name>: s3://example-bucket\n"})}),"\n",(0,s.jsxs)(n.p,{children:["You'll notice the PVC ",(0,s.jsx)(n.code,{children:"spark-driver-pvc"})," will be used by driver pod but Spark will create multiple ebs volumes for executors mapped to Storageclass ",(0,s.jsx)(n.code,{children:"emr-eks-karpenter-ebs-sc"}),". All dynamically created ebs volumes will be deleted once the job completes"]}),"\n",(0,s.jsx)(n.h3,{id:"running-sample-spark-job-using-fsx-for-lustre",children:"Running Sample Spark job using FSx for Lustre"}),"\n",(0,s.jsxs)(n.p,{children:["Amazon FSx for Lustre is a fully managed shared storage option built on the world\u2019s most popular high-performance file system. You can use FSx to store shuffle files and also to store intermediate data processing tasks in a data pipeline. You can read more about FSX for Lustre in ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html",children:"documentation"})," and learn how to use this storage with EMR on EKS in our ",(0,s.jsx)(n.a,{href:"https://aws.github.io/aws-emr-containers-best-practices/storage/docs/spark/fsx-lustre/",children:"best practices guide"})]}),"\n",(0,s.jsx)(n.p,{children:"In this example, you will learn how to deploy, configure and use FSx for Lustre as a shuffle storage. There are two ways to use FSx for Lustre"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"using static FSx for Lustre volumes"}),"\n",(0,s.jsx)(n.li,{children:"using dynamically created FSx for Lustre volumes"}),"\n"]}),"\n",(0,s.jsxs)(a.Z,{children:[(0,s.jsxs)(o.Z,{value:"fsx-static",lebal:"fsx-static",default:!0,children:[(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Execute Spark Job by using ",(0,s.jsx)(n.code,{children:"FSx for Lustre"})," with statically provisioned volume and compute optimized Karpenter Nodepool."]})}),(0,s.jsxs)(n.p,{children:["Fsx for Lustre Terraform module is disabled by default. Follow the ",(0,s.jsx)(n.a,{href:"#customizing-add-ons",children:"customizing add-ons"})," steps before running Spark jobs."]}),(0,s.jsx)(n.p,{children:"Execute the Spark job using the below shell script."}),(0,s.jsxs)(n.p,{children:["This script requires input parameters which can be extracted from ",(0,s.jsx)(n.code,{children:"terraform apply"})," output values."]}),(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsx)(n.p,{children:"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job."})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-static-pvc-shuffle-storage\n\n./fsx-static-spark.sh\n"})}),(0,s.jsx)(n.p,{children:"Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verify the job execution events"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods --namespace=emr-data-team-a -w\n"})}),(0,s.jsxs)(n.p,{children:["This will show the mounted ",(0,s.jsx)(n.code,{children:"/data"})," directory with FSx DNS name"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl exec -ti taxidata-exec-1 -c spark-kubernetes-executor -n emr-data-team-a -- df -h\n\nkubectl exec -ti taxidata-exec-1 -c spark-kubernetes-executor -n emr-data-team-a -- ls -lah /static\n"})})]}),(0,s.jsxs)(o.Z,{value:"fsx-dynamic",lebal:"fsx-dynamic",default:!0,children:[(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.strong,{children:["Execute Spark Job by using ",(0,s.jsx)(n.code,{children:"FSx for Lustre"})," with dynamically provisioned volume and compute optimized Karpenter Nodepool."]})}),(0,s.jsxs)(n.p,{children:["Fsx for Lustre Terraform module is disabled by default. Follow the ",(0,s.jsx)(n.a,{href:"#customizing-add-ons",children:"customizing add-ons"})," steps before running Spark jobs."]}),(0,s.jsxs)(n.p,{children:["Execute Spark Job by using ",(0,s.jsx)(n.code,{children:"FSx for Lustre"})," as a Shuffle storage for Driver and Executor pods with dynamically provisioned FSx filesystem and Persistent volume.\nExecute the Spark job using the below shell script."]}),(0,s.jsxs)(n.p,{children:["This script requires input parameters which can be extracted from ",(0,s.jsx)(n.code,{children:"terraform apply"})," output values."]}),(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsx)(n.p,{children:"This shell script downloads the test data to your local machine and uploads to S3 bucket. Verify the shell script before running the job."})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd analytics/terraform/emr-eks-karpenter/examples/fsx-for-lustre/fsx-dynamic-pvc-shuffle-storage\n\n./fsx-dynamic-spark.sh\n"})}),(0,s.jsx)(n.p,{children:"Karpetner may take between 1 and 2 minutes to spin up a new compute node as specified in the Nodepool templates before running the Spark Jobs.\nNodes will be drained with once the job is completed"}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verify the job execution events"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods --namespace=emr-data-team-a -w\n"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl exec -ti taxidata-exec-1 -c spark-kubernetes-executor -n emr-data-team-a -- df -h\n\nkubectl exec -ti taxidata-exec-1 -c spark-kubernetes-executor -n emr-data-team-a -- ls -lah /dynamic\n"})})]})]}),"\n",(0,s.jsx)(n.h3,{id:"running-sample-spark-job-using-apache-yunikorn-batch-scheduler",children:"Running Sample Spark job using Apache YuniKorn Batch Scheduler"}),"\n",(0,s.jsx)(n.p,{children:"Apache YuniKorn is an open-source, universal resource scheduler for managing distributed big data processing workloads such as Spark, Flink, and Storm. It is designed to efficiently manage resources across multiple tenants in a shared, multi-tenant cluster environment.\nSome of the key features of Apache YuniKorn include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexibility"}),": YuniKorn provides a flexible and scalable architecture that can handle a wide variety of workloads, from long-running services to batch jobs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic Resource Allocation"}),": YuniKorn uses a dynamic resource allocation mechanism to allocate resources to workloads on an as-needed basis, which helps to minimize resource wastage and improve overall cluster utilization."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Priority-based Scheduling"}),": YuniKorn supports priority-based scheduling, which allows users to assign different levels of priority to their workloads based on business requirements."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-tenancy"}),": YuniKorn supports multi-tenancy, which enables multiple users to share the same cluster while ensuring resource isolation and fairness."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pluggable Architecture"}),": YuniKorn has a pluggable architecture that allows users to extend its functionality with custom scheduling policies and pluggable components."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Apache YuniKorn is a powerful and versatile resource scheduler that can help organizations efficiently manage their big data workloads while ensuring high resource utilization and workload performance."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Apache YuniKorn Architecture"}),"\n",(0,s.jsx)(n.img,{alt:"Apache YuniKorn",src:t(5144).Z+"",width:"3060",height:"1683"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Apache YuniKorn Gang Scheduling with Karpenter"})}),"\n",(0,s.jsx)(n.p,{children:"Apache YuniKorn Scheduler add-on is disabled by default. Follow the steps to deploy the Apache YuniKorn add-on and execute the Spark job."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Update the ",(0,s.jsx)(n.code,{children:"analytics/terraform/emr-eks-karpenter/variables.tf"})," file with the following"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-terraform",children:'variable "enable_yunikorn" {\n  default     = true\n  description = "Enable Apache YuniKorn Scheduler"\n  type        = bool\n}\n'})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Execute ",(0,s.jsx)(n.code,{children:"terrafrom apply"})," again. This will deploy FSx for Lustre add-on and all the necessary resources."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-terraform",children:"terraform apply -auto-approve\n"})}),"\n",(0,s.jsxs)(n.p,{children:["This example demonstrates the ",(0,s.jsx)(n.a,{href:"https://yunikorn.apache.org/docs/user_guide/gang_scheduling/",children:"Apache YuniKorn Gang Scheduling"})," with Karpenter Autoscaler."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/karpenter-yunikorn-gangscheduling\n\n./execute_emr_eks_job.sh\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Verify the job execution"}),"\nApache YuniKorn Gang Scheduling will create pause pods for total number of executors requested."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get pods --namespace=emr-data-team-a -w\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Verify the driver and executor pods prefix with ",(0,s.jsx)(n.code,{children:"tg-"})," indicates the pause pods.\nThese pods will be replaced with the actual Spark Driver and Executor pods once the Nodes are scaled and ready by the Karpenter."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"img.png",src:t(9457).Z+"",width:"2962",height:"1932"})}),"\n",(0,s.jsxs)(i.Z,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Delta Lake Table Format"})}),children:[(0,s.jsx)(n.p,{children:"Delta Lake is a leading table format which is used to organize and store data.\nThe table format allows us to abstract different data files stored as objects as a singular dataset, a table."}),(0,s.jsx)(n.p,{children:"The source format provides a transactional and scalable layer, enabling efficient and easy-to-manage data processing.\nIt offer features such as"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ACID (Atomicity, Consistency, Isolation, and Durability) transactions"}),"\n",(0,s.jsx)(n.li,{children:"data merge operations"}),"\n",(0,s.jsx)(n.li,{children:"data versioning"}),"\n",(0,s.jsx)(n.li,{children:"processing performance"}),"\n"]}),(0,s.jsx)(n.p,{children:"Below quickstart examples showcases the features and usage of the delta lake table formats."}),(0,s.jsx)(a.Z,{children:(0,s.jsxs)(o.Z,{value:"deltalake",label:"insert & merge operations",default:!0,children:[(0,s.jsx)(n.p,{children:"In this example we will load a csv file into a delta lake table format by running Spark jobs on an EMR on EKS cluster."}),(0,s.jsx)(n.h3,{id:"prerequisites-1",children:"Prerequisites:"}),(0,s.jsxs)(n.p,{children:["The necessary EMR on EKS cluster has been provisioned as per instructions in the beginning of this page.\nThis script requires input parameters which can be extracted from ",(0,s.jsx)(n.code,{children:"terraform apply"})," output values.\nExecute the Spark job using the below shell script."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Navigate to folder and execute script:\n\ncd analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/deltalake\n./execute-deltacreate.sh\n"})}),(0,s.jsxs)(n.admonition,{type:"tip",children:[(0,s.jsx)(n.p,{children:"This shell script uploads test data and pyspark scripts to S3 bucket.\nSpecify the S3 bucket where you want to upload the artifacts and create the delta table."}),(0,s.jsx)(n.p,{children:"Verify successful job completion by navigating to the EMR on EKS virtual cluster and view the job status.\nFor job failures, you can navigate to the S3 bucket emr-on-eks-logs and drill-down to the jobs folder and investigate the spark driver stdout and stderr logs."})]}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Following artifacts are created in S3 once the script is executed and the EMR on EKS job is successfully completed."})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(9917).Z+"",width:"2240",height:"1192"})}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The data folder contains two csv files.(initial_emp.csv and update_emp.csv)"}),"\n",(0,s.jsx)(n.li,{children:"The scripts folder contains two pyspark scripts.(delta-create.py and delta-merge.py) for initial load and subsequent merge."}),"\n",(0,s.jsx)(n.li,{children:"The delta lake table is created in the delta\\delta_emp folder."}),"\n",(0,s.jsx)(n.li,{children:"There is also a symlink manifest file created in the delta\\delta_emp_symlink_format_manifest and registered to glue catalog for athena to query the initial table."}),"\n"]}),(0,s.jsx)(n.p,{children:"** Using Athena to query the created delta table. **"}),(0,s.jsx)(n.p,{children:"Athena, is a serverless query service offered by AWS.\nIt requires a symlink file for Delta tables registered to the Glue catalog.\nThis is required because Delta Lake uses a specific directory structure to store data and metadata.\nThe symlink file serves as a pointer to the latest version of the Delta log file. Without this symlink,\nAthena would not be able to determine the correct version of the metadata file to use for a given query"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Navigate to Athena query editor."}),"\n",(0,s.jsx)(n.li,{children:"The delta table should appear under the default database in AWSDataCatalog as shown below."}),"\n",(0,s.jsx)(n.li,{children:"Select the table and preview the data and verify if all records from the initial_emp.csv is shown."}),"\n"]}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(1054).Z+"",width:"2592",height:"1026"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Query output as table"})}),(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:t(7949).Z+"",width:"1794",height:"1140"})}),(0,s.jsx)(n.p,{children:"In the second example we will load an updated csv file into into the previously created delta lake table by running the merge Spark job."}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Navigate to folder and execute script:\n\ncd analytics/terraform/emr-eks-karpenter/examples/nvme-ssd/deltalake\n./execute-deltamerge.sh\n"})}),(0,s.jsx)(n.p,{children:"** Verify successful job completion. Re-run the query in Athena and verify data is merged (insert and updates) and shown correctly in delta lake table.**"})]})})]}),"\n",(0,s.jsx)(n.h2,{id:"run-interactive-workload-with-managed-endpoint",children:"Run Interactive Workload with Managed Endpoint"}),"\n",(0,s.jsxs)(n.p,{children:["Managed endpoint is a gateway that provides connectivity from EMR Studio to EMR on EKS so that you can run interactive workloads. You can find out more information about it ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/how-it-works.html",children:"here"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"creating-a-managed-endpoint",children:"Creating a managed endpoint"}),"\n",(0,s.jsx)(n.p,{children:"In this example, we will create a managed endpoint under one of the data teams."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Navigate to folder and execute script:\n\ncd analytics/terraform/emr-eks-karpenter/examples/managed-endpoints\n./create-managed-endpoint.sh\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Enter the EMR Virtual Cluster Id: 4ucrncg6z4nd19vh1lidna2b3\nProvide your EMR on EKS team (emr-data-team-a or emr-data-team-b): emr-eks-data-team-a\nEnter your AWS Region: us-west-2\nEnter a name for your endpoint: emr-eks-team-a-endpoint\nProvide an S3 bucket location for logging (i.e. s3://my-bucket/logging/): s3://<bucket-name>/logs\nEnter the EMR Execution Role ARN (i.e. arn:aws:00000000000000000:role/EMR-Execution-Role): arn:aws:iam::181460066119:role/emr-eks-karpenter-emr-data-team-a\n"})}),"\n",(0,s.jsx)(n.p,{children:"The script will provide the following:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"JSON configuration file for the Managed Endpoint"}),"\n",(0,s.jsxs)(n.li,{children:["Configuration settings:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Default 8G Spark Driver"}),"\n",(0,s.jsx)(n.li,{children:"CloudWatch monitoring, with logs stored in the S3 bucket provided"}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.li,{children:"Proper endpoint creation with appropriate security group to allow using Karpenter"}),"\n",(0,s.jsx)(n.li,{children:"Outputs: Managed Endpoint ID and Load Balancer ARN."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Once you have created a managed endpoint, you can follow the instructions ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-configure.html",children:"here"})," to configure EMR Studio and associate the Managed endpoint to a workspace."]}),"\n",(0,s.jsx)(n.h3,{id:"cleanup-of-endpoint-resources",children:"Cleanup of Endpoint resources"}),"\n",(0,s.jsx)(n.p,{children:"To delete the managed endpoint, simply run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"aws emr-containers delete-managed-endpoint --id <Managed Endpoint ID> --virtual-cluster-id <Virtual Cluster ID>\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,s.jsxs)(i.Z,{header:(0,s.jsx)(n.h2,{children:(0,s.jsx)(n.span,{children:"Cleanup"})}),children:[(0,s.jsxs)(n.p,{children:["This script will cleanup the environment using ",(0,s.jsx)(n.code,{children:"-target"})," option to ensure all the resources are deleted in correct order."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd analytics/terraform/emr-eks-karpenter && chmod +x cleanup.sh\n./cleanup.sh\n"})})]}),"\n",(0,s.jsx)(n.admonition,{type:"caution",children:(0,s.jsx)(n.p,{children:"To avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment"})})]})}function b(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(f,{...e})}):f(e)}},2425:(e,n,t)=>{t.d(n,{Z:()=>o});t(7294);var s=t(512);const r={tabItem:"tabItem__kUE"};var a=t(5893);function o(e){let{children:n,hidden:t,className:o}=e;return(0,a.jsx)("div",{role:"tabpanel",className:(0,s.Z)(r.tabItem,o),hidden:t,children:n})}},3913:(e,n,t)=>{t.d(n,{Z:()=>v});var s=t(7294),r=t(512),a=t(5944),o=t(6550),i=t(469),l=t(2434),d=t(2398),c=t(3088);function h(e){return s.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,s.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,s.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:s,default:r}}=e;return{value:n,label:t,attributes:s,default:r}}))}(t);return function(e){const n=(0,d.l)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const r=(0,o.k6)(),a=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,l._X)(a),(0,s.useCallback)((e=>{if(!a)return;const n=new URLSearchParams(r.location.search);n.set(a,e),r.replace({...r.location,search:n.toString()})}),[a,r])]}function x(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,a=u(e),[o,l]=(0,s.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const s=t.find((e=>e.default))??t[0];if(!s)throw new Error("Unexpected error: 0 tabValues");return s.value}({defaultValue:n,tabValues:a}))),[d,h]=m({queryString:t,groupId:r}),[x,g]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[r,a]=(0,c.Nk)(t);return[r,(0,s.useCallback)((e=>{t&&a.set(e)}),[t,a])]}({groupId:r}),f=(()=>{const e=d??x;return p({value:e,tabValues:a})?e:null})();(0,i.Z)((()=>{f&&l(f)}),[f]);return{selectedValue:o,selectValue:(0,s.useCallback)((e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),h(e),g(e)}),[h,g,a]),tabValues:a}}var g=t(2389);const f={tabList:"tabList_fbd4",tabItem:"tabItem_v5XY"};var b=t(5893);function j(e){let{className:n,block:t,selectedValue:s,selectValue:o,tabValues:i}=e;const l=[],{blockElementScrollPositionUntilNextRender:d}=(0,a.o5)(),c=e=>{const n=e.currentTarget,t=l.indexOf(n),r=i[t].value;r!==s&&(d(n),o(r))},h=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=l.indexOf(e.currentTarget)+1;n=l[t]??l[0];break}case"ArrowLeft":{const t=l.indexOf(e.currentTarget)-1;n=l[t]??l[l.length-1];break}}n?.focus()};return(0,b.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.Z)("tabs",{"tabs--block":t},n),children:i.map((e=>{let{value:n,label:t,attributes:a}=e;return(0,b.jsx)("li",{role:"tab",tabIndex:s===n?0:-1,"aria-selected":s===n,ref:e=>l.push(e),onKeyDown:h,onClick:c,...a,className:(0,r.Z)("tabs__item",f.tabItem,a?.className,{"tabs__item--active":s===n}),children:t??n},n)}))})}function y(e){let{lazy:n,children:t,selectedValue:r}=e;const a=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=a.find((e=>e.props.value===r));return e?(0,s.cloneElement)(e,{className:"margin-top--md"}):null}return(0,b.jsx)("div",{className:"margin-top--md",children:a.map(((e,n)=>(0,s.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function k(e){const n=x(e);return(0,b.jsxs)("div",{className:(0,r.Z)("tabs-container",f.tabList),children:[(0,b.jsx)(j,{...e,...n}),(0,b.jsx)(y,{...e,...n})]})}function v(e){const n=(0,g.Z)();return(0,b.jsx)(k,{...e,children:h(e.children)},String(n))}},769:(e,n,t)=>{t.d(n,{Z:()=>m});var s=t(7294),r=t(5697),a=t.n(r),o=t(512);const i="collapsibleContent_q3kw",l="header_QCEw",d="icon_PckA",c="content_qLC1",h="expanded_iGsi";var u=t(5893);function p(e){let{children:n,header:t}=e;const[r,a]=(0,s.useState)(!1);return(0,u.jsxs)("div",{className:i,children:[(0,u.jsxs)("div",{className:(0,o.Z)(l,{[h]:r}),onClick:()=>{a(!r)},children:[t,(0,u.jsx)("span",{className:(0,o.Z)(d,{[h]:r}),children:r?"\ud83d\udc47":"\ud83d\udc48"})]}),r&&(0,u.jsx)("div",{className:c,children:n})]})}p.propTypes={children:a().node.isRequired,header:a().node.isRequired};const m=p},1054:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/athena-query-1-03b2ff8a5c105ae57f7467bdb864ff9f.png"},7949:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/athena-query-results-10469fdfdd4a7d664284c41cc92d24eb.png"},9917:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/deltalake-s3-64065f06a765cdd04d6dc86cfb65a1e6.png"},5039:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/emr-eks-karpenter-aa9147ea1a119fb2da5f22404d4b2b83.png"},9457:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/karpenter-yunikorn-gang-schedule-358958a77e7a238ef9edea38f09a7b9d.png"},5144:(e,n,t)=>{t.d(n,{Z:()=>s});const s=t.p+"assets/images/yunikorn-56ae296071d7f89d5e0de755c1d026ca.png"}}]);