"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[2887],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>h});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},m="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),m=c(a),d=o,h=m["".concat(l,".").concat(d)]||m[d]||p[d]||r;return a?n.createElement(h,s(s({ref:t},u),{},{components:a})):n.createElement(h,s({ref:t},u))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,s=new Array(r);s[0]=d;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[m]="string"==typeof e?e:o,s[1]=i;for(var c=2;c<r;c++)s[c]=a[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},769:(e,t,a)=>{a.d(t,{Z:()=>d});var n=a(7294),o=a(5697),r=a.n(o),s=a(6010);const i="collapsibleContent_q3kw",l="header_QCEw",c="icon_PckA",u="content_qLC1",m="expanded_iGsi";function p(e){let{children:t,header:a}=e;const[o,r]=(0,n.useState)(!1);return n.createElement("div",{className:i},n.createElement("div",{className:(0,s.Z)(l,{[m]:o}),onClick:()=>{r(!o)}},a,n.createElement("span",{className:(0,s.Z)(c,{[m]:o})})),o&&n.createElement("div",{className:u},t))}p.propTypes={children:r().node.isRequired,header:r().node.isRequired};const d=p},4151:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var n=a(7462),o=(a(7294),a(3905));a(769);const r={sidebar_position:5,sidebar_label:"Observability for Kubeflow on EKS"},s="Monitor Machine Learning workflows with Kubeflow on Amazon EKS",i={unversionedId:"blueprints/ai-ml/monitoring-kubeflow",id:"blueprints/ai-ml/monitoring-kubeflow",title:"Monitor Machine Learning workflows with Kubeflow on Amazon EKS",description:"As part of day 2 operations, customers want to monitor their Infrastructure, Amazon EKS clusters and application components. AWS customers use Amazon EKS to run machine learning workloads. Containerization allows machine learning engineers to package and distribute models easily, while Kubernetes helps in deploying, scaling, and improving. In addition to monitoring the behavior of the Amazon EKS clusters, it\u2019s essential to monitor the behavior of machine learning workflows as well to ensure the operational resilience of workloads and platforms run by an organization.",source:"@site/docs/blueprints/ai-ml/monitoring-kubeflow.md",sourceDirName:"blueprints/ai-ml",slug:"/blueprints/ai-ml/monitoring-kubeflow",permalink:"/data-on-eks/docs/blueprints/ai-ml/monitoring-kubeflow",draft:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/blueprints/ai-ml/monitoring-kubeflow.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5,sidebar_label:"Observability for Kubeflow on EKS"},sidebar:"blueprints",previous:{title:"JupyterHub on EKS",permalink:"/data-on-eks/docs/blueprints/ai-ml/jupyterhub"},next:{title:"Job Schedulers on EKS",permalink:"/data-on-eks/docs/category/job-schedulers-on-eks"}},l={},c=[{value:"<strong>Architecture</strong>",id:"architecture",level:2},{value:"Solution Walkthrough",id:"solution-walkthrough",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Create an EKS Cluster",id:"create-an-eks-cluster",level:3},{value:"Installing <strong>Amazon Elastic Block Store (EBS) Container Storage Interface Driver</strong>",id:"installing-amazon-elastic-block-store-ebs-container-storage-interface-driver",level:3},{value:"Installing Kubeflow",id:"installing-kubeflow",level:3},{value:"Accessing Kubeflow Central Dashboard",id:"accessing-kubeflow-central-dashboard",level:3},{value:"Setup Amazon Managed Service for Prometheus",id:"setup-amazon-managed-service-for-prometheus",level:3},{value:"Setting up the AWS Distro for OpenTelemetry (ADOT) Collector to Ingest Metrics",id:"setting-up-the-aws-distro-for-opentelemetry-adot-collector-to-ingest-metrics",level:3},{value:"Amazon Managed Grafana Setup",id:"amazon-managed-grafana-setup",level:3},{value:"Query Kubeflow Metrics",id:"query-kubeflow-metrics",level:3},{value:"Creating a sample Machine Learning pipeline in Kubeflow",id:"creating-a-sample-machine-learning-pipeline-in-kubeflow",level:3},{value:"Visualizing Machine Learning pipeline metrics on Amazon Managed Grafana",id:"visualizing-machine-learning-pipeline-metrics-on-amazon-managed-grafana",level:3},{value:"Alerting Kubeflow workflows with Amazon Managed Grafana",id:"alerting-kubeflow-workflows-with-amazon-managed-grafana",level:3},{value:"Clean-up",id:"clean-up",level:2},{value:"Conclusion",id:"conclusion",level:2}],u={toc:c},m="wrapper";function p(e){let{components:t,...r}=e;return(0,o.kt)(m,(0,n.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"monitor-machine-learning-workflows-with-kubeflow-on-amazon-eks"},"Monitor Machine Learning workflows with Kubeflow on Amazon EKS"),(0,o.kt)("p",null,"As part of day 2 operations, customers want to monitor their Infrastructure, Amazon EKS clusters and application components. AWS customers use Amazon EKS to run machine learning workloads. Containerization allows machine learning engineers to package and distribute models easily, while Kubernetes helps in deploying, scaling, and improving. In addition to monitoring the behavior of the Amazon EKS clusters, it\u2019s essential to monitor the behavior of machine learning workflows as well to ensure the operational resilience of workloads and platforms run by an organization."),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://www.kubeflow.org/"},"Kubeflow")," is the open-source machine learning (ML) platform dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Kubeflow provides many components, including a central dashboard, multi-user Jupyter notebooks, Kubeflow Pipelines, KFServing, and Katib, as well as distributed training operators for ",(0,o.kt)("a",{parentName:"p",href:"https://www.tensorflow.org/"},"TensorFlow"),", ",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/"},"PyTorch"),", ",(0,o.kt)("a",{parentName:"p",href:"https://mxnet.apache.org/versions/1.9.1/"},"MXNet"),", and ",(0,o.kt)("a",{parentName:"p",href:"https://xgboost.readthedocs.io/en/stable/"},"XGBoost"),". Kubeflow components export metrics which provides insights into the health and function of Kubeflow on ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/eks/"},"Amazon Elastic Kubernetes Service (EKS)"),"."),(0,o.kt)("p",null,(0,o.kt)("a",{parentName:"p",href:"https://opentelemetry.io/docs/concepts/what-is-opentelemetry/"},"OpenTelemetry")," is a set of APIs, SDKs, and tools that are designed for the creation and management of telemetry data such as traces, metrics, and logs. ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/aws-observability/aws-otel-collector"},"AWS Distro for OpenTelemetry Collector (ADOT Collector)")," is an AWS-supported version of the upstream OpenTelemetry Collector that is fully compatible with AWS computing platforms, including EKS. It enables users to send telemetry data to AWS managed services such as Amazon CloudWatch, Amazon Managed Service for Prometheus, and AWS X-Ray. In this post, We\u2019ll show how you can configure an ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/eks/"},"Amazon Elastic Kubernetes Service (Amazon EKS)")," cluster with Kubeflow, ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/prometheus/"},"Amazon Managed Service for Prometheus"),", and",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/grafana/"},"Amazon Managed Grafana")," using ",(0,o.kt)("a",{parentName:"p",href:"https://aws-otel.github.io/docs/introduction"},"AWS Distro for OpenTelemetry (ADOT)")," for monitoring your Kubeflow machine learning workflows."),(0,o.kt)("h2",{id:"architecture"},(0,o.kt)("strong",{parentName:"h2"},"Architecture")),(0,o.kt)("p",null,"The following diagram shows the complete setup that we will walk through in this walk through:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(4785).Z,width:"1012",height:"974"})),(0,o.kt)("h2",{id:"solution-walkthrough"},"Solution Walkthrough"),(0,o.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("p",null,"You will need the following to complete the steps in this post:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"An ",(0,o.kt)("a",{parentName:"li",href:"https://awslabs.github.io/kubeflow-manifests/docs/deployment/prerequisites/"},"Ubuntu development environment")," with access to an AWS environment"),(0,o.kt)("li",{parentName:"ul"},"Install ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/okigan/awscurl"},"awscurl")," which is a curl-like tool with AWS Signature Version 4 request signing on your environment")),(0,o.kt)("p",null,"First, Let\u2019s start by setting a few environment variables:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"export KFL_EKS_CLUSTER=KFL-EKS-CLUSTER\nexport KFL_EKS_CLUSTER_V=1.25\nexport KFL_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nexport KFL_AWS_REGION=us-west-2 # Your AWS Region\nexport AWS_REGION=us-west-2 # Your AWS Region\nexport KFL_AMP_WORKSPACE_NAME=kubeflow-amp-workshop\nexport CLUSTER_NAME=KFL-EKS-CLUSTER\nexport CLUSTER_REGION=us-west-2\nexport KUBEFLOW_RELEASE_VERSION=v1.7.0\nexport AWS_RELEASE_VERSION=v1.7.0-aws-b1.0.1\n")),(0,o.kt)("p",null,"Next, let's start with installing prerequisites such as ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"},"AWS CLI version 2"),", ",(0,o.kt)("a",{parentName:"p",href:"https://eksctl.io/introduction/#installation"},"eksctl"),", ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html"},"kubectl"),", ",(0,o.kt)("a",{parentName:"p",href:"https://www.python.org/downloads/release/python-389/"},"python3.8"),", ",(0,o.kt)("a",{parentName:"p",href:"https://mikefarah.gitbook.io/yq/"},"yq"),", ",(0,o.kt)("a",{parentName:"p",href:"https://stedolan.github.io/jq/download/"},"jq"),", ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/okigan/awscurl"},"awscurl,"),(0,o.kt)("a",{parentName:"p",href:"https://kubectl.docs.kubernetes.io/installation/kustomize/"},"kustomize version 5+")," required to run the demonstration. Clone the ",(0,o.kt)("inlineCode",{parentName:"p"},"awslabs/kubeflow-manifests")," ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/awslabs/kubeflow-manifests"},"repo")," and checkout a release. Substitute the value for ",(0,o.kt)("inlineCode",{parentName:"p"},"AWS_RELEASE_VERSION")," with ",(0,o.kt)("inlineCode",{parentName:"p"},"v1.7.0-aws-b1.0.1")," and run the following command. Read more about ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/awslabs/kubeflow-manifests/blob/v1.3-branch/distributions/aws/examples/README.md#releases-and-versioning"},"releases and versioning")," policy to determine the right version for you for installing Kubeflow."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/awslabs/kubeflow-manifests.git && cd kubeflow-manifests\ngit checkout ${AWS_RELEASE_VERSION}\ngit clone --branch ${KUBEFLOW_RELEASE_VERSION} https://github.com/kubeflow/manifests.git upstream\n`make install``-``tools`\n")),(0,o.kt)("h3",{id:"create-an-eks-cluster"},"Create an EKS Cluster"),(0,o.kt)("p",null,"Let\u2019s create an Amazon EKS cluster using ",(0,o.kt)("inlineCode",{parentName:"p"},"eksctl"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"## eksctl Cluster creation command for EKS cluster.\neksctl create cluster \\\n  --name $KFL_EKS_CLUSTER \\\n  --version $KFL_EKS_CLUSTER_V \\\n  --region $KFL_AWS_REGION \\\n  --nodegroup-name linux-nodes \\\n  --node-type m5.xlarge \\\n  --nodes 5 \\\n  --nodes-min 1 \\\n  --nodes-max 10 \\\n  --managed \\\n  --with-oidc\n")),(0,o.kt)("h3",{id:"installing-amazon-elastic-block-store-ebs-container-storage-interface-driver"},"Installing ",(0,o.kt)("strong",{parentName:"h3"},"Amazon Elastic Block Store (EBS) Container Storage Interface Driver")),(0,o.kt)("p",null,"A ",(0,o.kt)("a",{parentName:"p",href:"https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/"},"Container Storage Interface (CSI) driver")," is needed in order to get your ",(0,o.kt)("inlineCode",{parentName:"p"},"PersisentVolumeClaims")," served by a ",(0,o.kt)("inlineCode",{parentName:"p"},"PersistentVolume"),". Please run the following commands to create Amazon EBS CSI driver IAM role and add EBS CSI add-on :"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"eksctl create iamserviceaccount \\\n  --name ebs-csi-controller-sa \\\n  --namespace kube-system \\\n  --cluster $KFL_EKS_CLUSTER \\\n  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n  --approve \\\n  --role-only \\\n  --role-name AmazonEKS_EBS_CSI_DriverRole\n\neksctl create addon \\\n    --name aws-ebs-csi-driver \\\n    --cluster $KFL_EKS_CLUSTER \\\n    --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n    --force\n")),(0,o.kt)("h3",{id:"installing-kubeflow"},"Installing Kubeflow"),(0,o.kt)("p",null,"You can install all Kubeflow official components (residing under ",(0,o.kt)("inlineCode",{parentName:"p"},"apps"),") and all common services (residing under ",(0,o.kt)("inlineCode",{parentName:"p"},"common"),") using the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"make deploy-kubeflow INSTALLATION_OPTION=kustomize DEPLOYMENT_OPTION=vanilla\n")),(0,o.kt)("p",null,"It takes around 5 minutes for all components to get installed. Once everything is installed successfully, you can access the Kubeflow Central Dashboard. ",(0,o.kt)("a",{parentName:"p",href:"https://awslabs.github.io/kubeflow-manifests/"},"Kubeflow on AWS")," page has more information for learning open source distribution of ",(0,o.kt)("a",{parentName:"p",href:"https://www.kubeflow.org/"},"Kubeflow"),"  on AWS."),(0,o.kt)("p",null,"After installation, it will take some time for all Pods to become ready. Make sure all Pods are ready before trying to connect, otherwise you might get unexpected errors. To check that all Kubeflow-related Pods are ready, use the following command:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl get pods -A -o json | jq -r \'.items[] | select(.metadata.namespace=="cert-manager" or "istio-system" or "auth" or "knative-eventing" or "knative-serving" or "kubeflow" or "kubeflow-user-example-com") | .metadata.namespace + "|" + .metadata.name + "|" + .status.phase\'\n')),(0,o.kt)("h3",{id:"accessing-kubeflow-central-dashboard"},"Accessing Kubeflow Central Dashboard"),(0,o.kt)("p",null,"Kubeflow can be accessed via port-forward and this enables you to get started quickly without imposing any requirements on your environment. Run the following to port-forward Istio's Ingress-Gateway to local port ",(0,o.kt)("inlineCode",{parentName:"p"},"8080"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n")),(0,o.kt)("p",null,"After running the command, you can access the Kubeflow Central Dashboard by doing the following:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Dex is an OpenID Connect Identity (OIDC) with multiple authentication backends. Open your browser and visit ",(0,o.kt)("inlineCode",{parentName:"li"},"http://localhost:8080")," and You should get the Dex login screen."),(0,o.kt)("li",{parentName:"ol"},"Login with the default user's credential. The default email address is ",(0,o.kt)("inlineCode",{parentName:"li"},"user@example.com")," and the default password is ",(0,o.kt)("inlineCode",{parentName:"li"},"12341234"),".")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(3681).Z,width:"3554",height:"1550"})),(0,o.kt)("h3",{id:"setup-amazon-managed-service-for-prometheus"},"Setup Amazon Managed Service for Prometheus"),(0,o.kt)("p",null,"A workspace in ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/prometheus/"},"Amazon Managed Service for Prometheus")," is a logical and isolated Prometheus server dedicated to Prometheus resources such as metrics. A workspace supports fine-grained access control for authorizing its management such as update, list, describe, delete, and the ingestion and querying of metrics."),(0,o.kt)("p",null,"Please open a new terminal window and setup all environment variables as you did in start of the demonstration. Please use the below command to create an Amazon Managed Service for Prometheus workspace."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws amp create-workspace \\\n  --alias $KFL_AMP_WORKSPACE_NAME \\\n  --region $KFL_AWS_REGION\n")),(0,o.kt)("p",null,"The Amazon Managed Service for Prometheus  workspace should be created in just a few seconds."),(0,o.kt)("p",null,"As a best practice, create a ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/vpc/latest/privatelink/create-interface-endpoint.html"},"VPC endpoint"),"for Amazon Managed Service for Prometheus in VPC running your Amazon EKS cluster. Please visit ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-and-interface-VPC.html"},"Using Amazon Managed Service for Prometheus with interface VPC endpoints")," for more information."),(0,o.kt)("h3",{id:"setting-up-the-aws-distro-for-opentelemetry-adot-collector-to-ingest-metrics"},"Setting up the AWS Distro for OpenTelemetry (ADOT) Collector to Ingest Metrics"),(0,o.kt)("p",null,"Amazon Managed Service for Prometheus does not directly scrape operational metrics from containerized workloads in a Kubernetes or ECS cluster. It requires users to deploy a collection agent such as Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector in their cluster to perform this task."),(0,o.kt)("p",null,"One of the easiest ways to collect Prometheus metrics from Amazon EKS workloads is by using the ",(0,o.kt)("a",{parentName:"p",href:"https://aws-otel.github.io/docs/getting-started/collector"},"AWS Distro for OpenTelemetry (ADOT) collector"),". Customers can deploy the ADOT Collector in a variety of deployment models and easily manage configuration using the ADOT Operator. The ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/opentelemetry.html"},"ADOT Operator is also available as an EKS Add-On"),"for easier deployment and management. Read our ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/containers/metrics-and-traces-collection-using-amazon-eks-add-ons-for-aws-distro-for-opentelemetry/"},"launch blog"),"to learn about this feature."),(0,o.kt)("p",null,"The best way to provision permissions for resources running on EKS clusters is through ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html"},"IRSA"),". The command below will use AWS CloudFormation to create a K8s namespace called ",(0,o.kt)("inlineCode",{parentName:"p"},"prometheus"),", create a K8s Service Account called ",(0,o.kt)("inlineCode",{parentName:"p"},"amp-iamproxy-ingest-role"),", create a new IAM Role with the ",(0,o.kt)("inlineCode",{parentName:"p"},"AmazonPrometheusRemoteWriteAccess")," policy attached to it. It will also create a trust policy between the EKS cluster's IAM OpenID Connect Provider (OIDC) and the created Service Account. See ",(0,o.kt)("a",{parentName:"p",href:"https://eksctl.io/usage/iamserviceaccounts/"},"this link")," to learn more about this command."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create namespace prometheus\neksctl create iamserviceaccount \\\n  --name amp-iamproxy-ingest-role \\\n  --namespace prometheus \\\n  --cluster $KFL_EKS_CLUSTER \\\n  --attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \\\n  --approve \\--override-existing-serviceaccounts\n")),(0,o.kt)("p",null,"Next, we will grant permissions to Amazon EKS add-ons to install ADOT and then we will installing the ADOT Add-on :"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl apply -f https://amazon-eks.s3.amazonaws.com/docs/addons-otel-permissions.yaml\naws eks create-addon \\\n  --addon-name adot \\\n  --cluster-name $KFL_EKS_CLUSTER\n")),(0,o.kt)("p",null,"Now, wait for 30 seconds and execute the following command. You should see ",(0,o.kt)("inlineCode",{parentName:"p"},'"ACTIVE"')," as result indicating that the add-on is installed successfully."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"aws eks describe-addon \\\n  --addon-name adot \\\n  --cluster-name $KFL_EKS_CLUSTER  | jq .addon.status\n")),(0,o.kt)("p",null,"Next, we will Install the OTel Collector Custom Resource Definition(CRD) and then we will configure the ADOT collector to push metrics to Amazon Managed Service for Prometheus endpoint."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"KFL_WORKSPACE_ID=$(aws amp list-workspaces \\\n  --alias $KFL_AMP_WORKSPACE_NAME \\\n  --region=${KFL_AWS_REGION} \\\n  --query 'workspaces[0].[workspaceId]' \\\n  --output text)\nKFL_AMP_ENDPOINT_URL=$(aws amp describe-workspace \\\n  --workspace-id $KFL_WORKSPACE_ID | jq .workspace.prometheusEndpoint -r)\nKFL_AMP_REMOTE_WRITE_URL=${KFL_AMP_ENDPOINT_URL}api/v1/remote_write\ncurl -O https://raw.githubusercontent.com/aws-samples/one-observability-demo/main/PetAdoptions/cdk/pet_stack/resources/otel-collector-prometheus.yaml\nsed -i -e s/AWS_REGION/$KFL_AWS_REGION/g otel-collector-prometheus.yaml\nsed -i -e s^AMP_WORKSPACE_URL^$KFL_AMP_REMOTE_WRITE_URL^g otel-collector-prometheus.yaml\nkubectl apply -f ./otel-collector-prometheus.yaml\n")),(0,o.kt)("p",null,"Now, lets verify that the ADOT collector is running and you should see a result like the one below showing that the collector has been successfully installed and being ready."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get all -n prometheus\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"NAME                                           READY   STATUS    RESTARTS   AGEpod/observability-collector-5774bbc68d-7nj54   1/1     Running   0          59s\n\nNAME                                         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/observability-collector-monitoring   ClusterIP   10.100.114.1   <none>        8888/TCP   59s\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/observability-collector   1/1     1            1           59s\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/observability-collector-5774bbc68d   1         1         1       59s\n")),(0,o.kt)("p",null,"Now you have successfully deployed the ADOT Collector to collect metrics from the EKS cluster and send it to the Amazon Managed Service for Prometheus workspace you created. To test whether Amazon Managed Service for Prometheus received the metrics, use ",(0,o.kt)("inlineCode",{parentName:"p"},"awscurl"),". This tool enables you to send HTTP requests through the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from Amazon Managed Service for Prometheus. For instructions on installing awscurl, see ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/okigan/awscurl"},"awscurl"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'awscurl --service="aps" \\\n --region="$KFL_AWS_REGION" "https://aps-workspaces.$KFL_AWS_REGION.amazonaws.com/workspaces/$KFL_WORKSPACE_ID/api/v1/query?query=istio_requests_total"\n')),(0,o.kt)("p",null,"Your results should look similar to shown below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'{\n    "status": "success",\n    "data": {\n        "resultType": "vector",\n        "result": [\n            {\n                "metric": {\n                    "__name__": "istio_requests_total",\n                    "app": "istio-ingressgateway",\n                    "chart": "gateways",\n                    ....................................\n                    ....................................\n                    "version": "v1"\n                },\n                "value": [\n                    1647974689.212,\n                    "1"\n                ]\n            }\n        ]\n    }\n}\n')),(0,o.kt)("h3",{id:"amazon-managed-grafana-setup"},"Amazon Managed Grafana Setup"),(0,o.kt)("p",null,"Two steps are necessary for setting up AWS IAM Identity Center, setting up and logging in to Amazon Managed Grafana, and querying metrics from Amazon Managed Service for Prometheus workspace from the post. To set up Authentication and Authorization, follow the instructions in the ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/grafana/latest/userguide/AMG-manage-users-and-groups-AMG.html"},"Amazon Managed Grafana User Guide")," for enabling AWS IAM Identity Center. Second, setup the data source for ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/grafana/latest/userguide/AMP-adding-AWS-config.html"},"Amazon Managed Service for Prometheus"),". You may also reference ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/mt/monitor-istio-on-eks-using-amazon-managed-prometheus-and-amazon-managed-grafana/#:~:text=AWS%20Single%20Sign%2DOn%20(SSO)"},"Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana")," blog, starting from the AWS Single Sign-On (SSO) section for Amazon Managed Grafana setup."),(0,o.kt)("h3",{id:"query-kubeflow-metrics"},"Query Kubeflow Metrics"),(0,o.kt)("p",null,"Next lets navigate to Amazon Managed Grafana console and import Grafana dashboards which allows us to visualize metrics from Istio environment. Go to the ",(0,o.kt)("inlineCode",{parentName:"p"},"plus")," sign on the left navigation bar and select ",(0,o.kt)("inlineCode",{parentName:"p"},"Import")," as shown below:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(4442).Z,width:"1778",height:"531"})),(0,o.kt)("p",null,"In the Import screen, type ",(0,o.kt)("inlineCode",{parentName:"p"},"7630 (Istio Workload Dashboard)")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"Import via grafana.com")," textbox and click ",(0,o.kt)("inlineCode",{parentName:"p"},"Load. "),"Select the Prometheus data source in the drop down at the bottom and click on ",(0,o.kt)("inlineCode",{parentName:"p"},"Import"),". Once complete, you will be able to see the Grafana dashboard showing metrics from the ",(0,o.kt)("inlineCode",{parentName:"p"},"Istio Workload Dashboard")," through Prometheus data source as shown below:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(5049).Z,width:"3800",height:"1798"})),(0,o.kt)("h3",{id:"creating-a-sample-machine-learning-pipeline-in-kubeflow"},"Creating a sample Machine Learning pipeline in Kubeflow"),(0,o.kt)("p",null,"Now that we have configured Amazon Managed Grafana with the Prometheus data source within our cluster, we can initiate a Machine Learning pipeline in Kubeflow, and be able to display metrics on the Grafana dashboards."),(0,o.kt)("p",null,"Before we create the notebook to use Kubeflow Pipelines SDK, we have to supply a token so that the notebook can authenticate with the Kubeflow Pipelines API. To do so, run the following command to create a Pod to mount a token volume:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},'cat <<EOF | kubectl apply -f -\napiVersion: kubeflow.org/v1alpha1\nkind: PodDefault\nmetadata:\n  name: access-ml-pipeline\n  namespace: kubeflow-user-example-com\nspec:\n  desc: Allow access to Kubeflow Pipelines\n  selector:\n    matchLabels:\n      access-ml-pipeline: "true"\n  volumes:\n    - name: volume-kf-pipeline-token\n      projected:\n        sources:\n          - serviceAccountToken:\n              path: token\n              expirationSeconds: 7200\n              audience: pipelines.kubeflow.org\n  volumeMounts:\n    - mountPath: /var/run/secrets/kubeflow/pipelines\n      name: volume-kf-pipeline-token\n      readOnly: true\n  env:\n    - name: KF_PIPELINES_SA_TOKEN_PATH\n      value: /var/run/secrets/kubeflow/pipelines/token\nEOF\n')),(0,o.kt)("p",null,"Now, access Kubeflow dashboard as described in the previous sections, via port-forwarding. Select ",(0,o.kt)("inlineCode",{parentName:"p"},"Notebooks"),", and create a new CPU-based notebook, using the following configurations:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"name: ",(0,o.kt)("inlineCode",{parentName:"li"},"ml-training-notebook")),(0,o.kt)("li",{parentName:"ul"},"docker image: ",(0,o.kt)("inlineCode",{parentName:"li"},"kubeflow-on-aws/notebook-servers/jupyter-pytorch:2.0.0-cpu-py310-ubuntu20.04-ec2-v1.0")),(0,o.kt)("li",{parentName:"ul"},"Requested CPUs: ",(0,o.kt)("inlineCode",{parentName:"li"},"1")),(0,o.kt)("li",{parentName:"ul"},"Requested memory in Gi: ",(0,o.kt)("inlineCode",{parentName:"li"},"5")),(0,o.kt)("li",{parentName:"ul"},"Advanced Options \u2192 Configurations: ",(0,o.kt)("inlineCode",{parentName:"li"},"Allow access to Kubeflow Pipelines"),"  (This configuration is the token we have generated above)")),(0,o.kt)("p",null,"With all other configurations as defaults, you should be able to see the notebook generate successfully and show up on the Kubeflow dashboard."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(5433).Z,width:"2548",height:"381"})),(0,o.kt)("p",null,"You can also verify that the notebook is created by verifying the Kubernetes resources being created:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods -n kubeflow-user-example-com --field-selector=status.phase==Running\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"NAME                                               READY   STATUS     RESTARTS   AGE\nml-pipeline-ui-artifact-5b7794c7b5-5hkqf           2/2     Running   0          100m\nml-pipeline-visualizationserver-85c6d6cc9f-vs24x   2/2     Running   0          100m\nml-training-notebook-0                             2/2     Running   0          11m\n")),(0,o.kt)("p",null,"You will be able to access the JupyterLab notebook by clicking CONNECT. This will open up a new JupyterLab window:"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(660).Z,width:"2553",height:"842"})),(0,o.kt)("p",null,"We will run a simple pipeline training notebook that uses Kubeflow Pipelines, from an existing ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/aws-samples/aws-deeplearning-labs"},"AWS Deep Learning sample repository"),". This is a simple model training to predict the taxi fare of Chicago cabs, and demonstrates continuous training using a recursive loop. It triggers a Kubeflow pipeline to train the initial model and then gradually trains the model until evaluation metrics are good enough. Lets run the Kubeflow pipeline using the following steps on the console :"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Clone the following repo by selecting ",(0,o.kt)("inlineCode",{parentName:"li"},"Git -> Clone a Repository")," from the top navigation bar and paste ",(0,o.kt)("inlineCode",{parentName:"li"},"https://github.com/aws-samples/aws-deeplearning-labs")," and press enter."),(0,o.kt)("li",{parentName:"ul"},"Open the following notebook from the directory view in the left pane: ",(0,o.kt)("inlineCode",{parentName:"li"},"aws-deeplearning-labs/workshop/pytorch-distributed-training/STEP2_simple_xgboost_training_pipeline.ipynb"),"."),(0,o.kt)("li",{parentName:"ul"},"Run all the cells of the model by selecting ",(0,o.kt)("inlineCode",{parentName:"li"},"Kernel -> Restart Kernel and Run All Cells")," from the top menu")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(2786).Z,width:"3448",height:"1742"})),(0,o.kt)("h3",{id:"visualizing-machine-learning-pipeline-metrics-on-amazon-managed-grafana"},"Visualizing Machine Learning pipeline metrics on Amazon Managed Grafana"),(0,o.kt)("p",null,"Using the Amazon Managed Grafana, we can show the resource utilization from our Machine Learning Pipelines with the same method we used to look above: using the ",(0,o.kt)("inlineCode",{parentName:"p"},"Istio Workload Dashboard")," (7630). Select the following to monitor your resources for this particular ML pipeline:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Datasource: ",(0,o.kt)("inlineCode",{parentName:"li"},"your prometheus workspace name")),(0,o.kt)("li",{parentName:"ul"},"Namespace: ",(0,o.kt)("inlineCode",{parentName:"li"},"kubeflow-user-example-com")),(0,o.kt)("li",{parentName:"ul"},"Workload: ",(0,o.kt)("inlineCode",{parentName:"li"},"ml-training-notebook"))),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Mon-Kubeflow",src:a(4905).Z,width:"2499",height:"1152"})),(0,o.kt)("h3",{id:"alerting-kubeflow-workflows-with-amazon-managed-grafana"},"Alerting Kubeflow workflows with Amazon Managed Grafana"),(0,o.kt)("p",null,"As we configure workflows with Kubeflow, alerting is a mechanism we can employ to alert on specific situations. By quickly identifying unintended changes in your workflow and notifying the same using alerts, you can take actions to minimize disruptions to your services. Amazon Managed Grafana supports multiple notification channels such as SNS, Slack, PagerDuty etc to which you can send alerts notifications. ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/grafana/latest/userguide/alerts-overview.html"},"Alerts")," page will show you more information on how to setup alerts in Amazon Managed Grafana. You learn about setting up alerts from Amazon Managed Grafana to ",(0,o.kt)("a",{parentName:"p",href:"https://slack.com/"},"Slack")," from  our ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/mt/monitoring-hybrid-environments-using-amazon-managed-service-for-grafana/"},"Monitoring hybrid environments using Amazon Managed Grafana")," blog. Also check our Blog on ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/mt/monitor-istio-on-eks-using-amazon-managed-prometheus-and-amazon-managed-grafana/"},"Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana")," which will show you on triggering Amazon Managed Grafana alerts to ",(0,o.kt)("a",{parentName:"p",href:"https://www.pagerduty.com/"},"PagerDuty"),"."),(0,o.kt)("h2",{id:"clean-up"},"Clean-up"),(0,o.kt)("p",null,"Use the following commands to clean up the created AWS resources for this demonstration:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"# Clean up ADOT Collector and Prometheus.\nkubectl delete -f https://amazon-eks.s3.amazonaws.com/docs/addons-otel-permissions.yaml\nkubectl delete -f ./otel-collector-prometheus.yaml\nrm -rf ./otel-collector-prometheus.yaml\n\naws eks delete-addon \\\n    --addon-name adot \\\n    --cluster-name $KFL_EKS_CLUSTER\n\naws amp delete-workspace \\\n  --workspace-id $KFL_WORKSPACE_ID \\\n  --region $KFL_AWS_REGION\n\neksctl delete iamserviceaccount \\\n  --name amp-iamproxy-ingest-role \\\n  --namespace prometheus \\\n  --cluster $KFL_EKS_CLUSTER\n\nkubectl delete namespace prometheus\n\n# Cleaning up kubeflow installation components\nmake delete-kubeflow INSTALLATION_OPTION=kustomize DEPLOYMENT_OPTION=vanilla\ncd ..\nrm -rf kubeflow-manifests\n\neksctl delete iamserviceaccount \\\n  --name ebs-csi-controller-sa \\\n  --namespace kube-system \\\n  --cluster $KFL_EKS_CLUSTER\n\naws eks delete-addon \\\n    --addon-name aws-ebs-csi-driver \\\n    --cluster-name $KFL_EKS_CLUSTER\n\n# Cleaning up Amazon EKS Cluster.\neksctl delete cluster --region $AWS_REGION --name $KFL_EKS_CLUSTER\n")),(0,o.kt)("p",null,"Finally navigate to Amazon Managed Grafana console to delete the created Grafana workspace."),(0,o.kt)("h2",{id:"conclusion"},"Conclusion"),(0,o.kt)("p",null,"This post demonstrated the detailed steps on how you can setup Amazon EKS cluster with Kubeflow, Amazon Managed Service for Prometheus and Amazon Managed Grafana to monitor your Kubeflow machine learning workflows.  "),(0,o.kt)("p",null,"It is also important to have a centralized incident management process to keep systems running smoothly. You can view more details on alerting in and various supported providers at ",(0,o.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/grafana/latest/userguide/alert-notifications.html"},"alert notifications")," for Amazon Managed Grafana. You can also check out previous blogs posts ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/mt/using-amazon-managed-service-for-prometheus-alert-manager-to-receive-alerts-with-pagerduty/"},"Amazon Managed Service for Prometheus Alert Manager to receive alerts with PagerDuty")," and ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/mt/how-to-integrate-amazon-managed-service-for-prometheus-with-slack/"},"how to integrate Amazon Managed Service for Prometheus with Slack")," to see how you can setup alerting with Amazon Managed Service for Prometheus."),(0,o.kt)("p",null,"For further reading on Kubeflow deployment and monitoring on Amazon EKS, check out ",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/blogs/machine-learning/build-and-deploy-a-scalable-machine-learning-system-on-kubernetes-with-kubeflow-on-aws/"},"Build and deploy a scalable machine learning system on Kubernetes with Kubeflow on AWS")," and ",(0,o.kt)("a",{parentName:"p",href:"https://awslabs.github.io/kubeflow-manifests/docs/deployment/add-ons/cloudwatch/guide/"},"CloudWatch add-on for Kubeflow."),"."))}p.isMDXComponent=!0},4785:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-1-fd998378bf1f1fc7e9045c9a225ebaf2.jpg"},3681:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-2-abb24d176d064dccf8e46eb08429d685.jpg"},4442:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-3-420e87a81bcc833283ce52e1e6d4926a.jpg"},5049:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-4-104e34dab44eacd046137923ff15929d.jpg"},5433:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-5-fdac1c7e17f72a11cf464e5ca84c1304.jpg"},660:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-6-ede25e58ae6bce6228f7218acdbf4699.jpg"},2786:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-7-68da0bee12946efad7ce90c7a0a750a9.jpg"},4905:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/mon-kubeflow-8-92525975ca2682b286063def15d20c51.jpg"}}]);